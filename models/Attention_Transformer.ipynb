{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-21T09:49:05.567368Z",
     "iopub.status.busy": "2025-08-21T09:49:05.566998Z",
     "iopub.status.idle": "2025-08-21T09:49:05.574323Z",
     "shell.execute_reply": "2025-08-21T09:49:05.573156Z",
     "shell.execute_reply.started": "2025-08-21T09:49:05.567342Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import face_alignment\n",
    "\n",
    "# Load pretrained landmark model (68 points, 2D)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "fa = face_alignment.FaceAlignment(face_alignment.LandmarksType.TWO_D, device=device, flip_input=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_nose_mask(image_path, save_path, log):\n",
    "    try:\n",
    "        img = cv2.imread(image_path)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "        preds = fa.get_landmarks(img_rgb)\n",
    "        if preds is None:\n",
    "            return False\n",
    "            \n",
    "    except Warning as w:  # Catch warnings as exceptions\n",
    "        return False\n",
    "\n",
    "    landmarks = preds[0]\n",
    "    nose_points = landmarks[27:36]  # indexes 27–35 in 0-based Python index\n",
    "\n",
    "    mask = np.zeros(img.shape[:2], dtype=np.uint8)\n",
    "    cv2.fillConvexPoly(mask, np.int32(nose_points), 255)\n",
    "\n",
    "    cv2.imwrite(save_path, mask)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "input_folder = \"/kaggle/input/ilab-facial-data/facial_images/processed2/train/A\"\n",
    "test_folder = \"/kaggle/input/ilab-facial-data/facial_images/processed2/val/A\"\n",
    "mask_folder = \"/kaggle/working/nose_mask\"\n",
    "test_mask_folder = \"/kaggle/working/test_nose_mask\"\n",
    "os.makedirs(mask_folder, exist_ok=True)\n",
    "os.makedirs(test_mask_folder, exist_ok=True)\n",
    "\n",
    "warnings.filterwarnings(\"error\", message=\"No faces were detected.\")\n",
    "\n",
    "for file in os.listdir(input_folder):\n",
    "    if file.lower().endswith(('.jpeg', '.png')):\n",
    "        create_nose_mask(os.path.join(input_folder, file), os.path.join(mask_folder, file), logging)\n",
    "\n",
    "for file in os.listdir(test_folder):\n",
    "    if file.lower().endswith(('.jpeg', '.png')):\n",
    "        create_nose_mask(os.path.join(test_folder, file), os.path.join(test_mask_folder, file), logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !ls -lh /kaggle/working/nose_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def img_to_patches(x, patch_size):\n",
    "    # x: (B, C, H, W)\n",
    "    B, C, H, W = x.shape\n",
    "    assert H % patch_size == 0 and W % patch_size == 0\n",
    "    ph, pw = patch_size, patch_size\n",
    "    nh, nw = H // ph, W // pw\n",
    "    x = x.reshape(B, C, nh, ph, nw, pw)\n",
    "    x = x.permute(0,2,4,3,5,1).reshape(B, nh*nw, ph*pw*C)  # (B, N, patch_dim)\n",
    "    return x, (nh, nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def patches_to_img(patches, patch_size, nh_nw, C):\n",
    "    # patches: (B, N, patch_dim)\n",
    "    B, N, D = patches.shape\n",
    "    nh, nw = nh_nw\n",
    "    ph = pw = patch_size\n",
    "    x = patches.reshape(B, nh, nw, ph, pw, C).permute(0,5,1,3,2,4).reshape(B, C, nh*ph, nw*pw)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def mask_to_patch_mask(mask, patch_size):\n",
    "    # mask: (B,1,H,W) binary [0,1]\n",
    "    B, _, H, W = mask.shape\n",
    "    ph = pw = patch_size\n",
    "    nh, nw = H//ph, W//pw\n",
    "    mask = mask.reshape(B, 1, nh, ph, nw, pw)\n",
    "    mask = mask.mean(dim=(3,5))  # (B,1,nh,nw)\n",
    "    patch_mask = (mask.view(B, nh*nw) > 0.1).float()  # (B, N)\n",
    "    return patch_mask  # 1 where patch contains nos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def validate_files(img_dir, mask_dir, target_dir):\n",
    "    img_dir, mask_dir, target_dir = map(Path, (img_dir, mask_dir, target_dir))\n",
    "    valid_files = []\n",
    "    for f in os.listdir(img_dir):\n",
    "        if f.startswith(\"pre\"):\n",
    "            target_f = f.replace(\"pre\", \"post\", 1)  # replace only first occurrence\n",
    "            if (img_dir/f).exists() and (mask_dir/f).exists() and (target_dir/target_f).exists():\n",
    "                valid_files.append((f, target_f))  # store input-target pair\n",
    "    return valid_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class NoseFolderDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, target_dir, size=256):\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.mask_dir = Path(mask_dir)\n",
    "        self.target_dir = Path(target_dir)\n",
    "        self.size = size\n",
    "        self.file_pairs = validate_files(img_dir, mask_dir, target_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname, target_f = self.file_pairs[idx]\n",
    "\n",
    "        img = cv2.imread(str(self.img_dir / fname))\n",
    "        mask = cv2.imread(str(self.mask_dir / fname), cv2.IMREAD_GRAYSCALE)\n",
    "        target = cv2.imread(str(self.target_dir / target_f))\n",
    "\n",
    "        # resize\n",
    "        img = cv2.resize(img, (self.size, self.size))\n",
    "        mask = cv2.resize(mask, (self.size, self.size))\n",
    "        target = cv2.resize(target, (self.size, self.size))\n",
    "\n",
    "        # tensors\n",
    "        img = torch.from_numpy(img).permute(2, 0, 1).float() / 255.\n",
    "        mask = torch.from_numpy(mask).unsqueeze(0).float() / 255.\n",
    "        target = torch.from_numpy(target).permute(2, 0, 1).float() / 255.\n",
    "\n",
    "        x = torch.cat([img, mask], dim=0)\n",
    "\n",
    "        return {\"input\": x, \"target\": target, \"mask\": mask, \"input_file\": str(self.img_dir / fname), \"mask_file\": str(self.mask_dir / fname), \"target_file\": str(self.target_dir / target_f)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, in_ch, embed_dim, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.proj = nn.Linear((patch_size*patch_size*in_ch), embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        patches, (nh, nw) = img_to_patches(x, self.patch_size)  # (B, N, patch_dim)\n",
    "        x = self.proj(patches)  # (B, N, embed_dim)\n",
    "        return x, (nh, nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PatchUnembed(nn.Module):\n",
    "    def __init__(self, out_ch, patch_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.rev = nn.Linear(embed_dim, patch_size*patch_size*out_ch)\n",
    "\n",
    "    def forward(self, x, nh_nw, out_ch):\n",
    "        # x: (B, N, embed_dim)\n",
    "        patches = self.rev(x)  # (B, N, patch_dim)\n",
    "        img = patches_to_img(patches, self.patch_size, nh_nw, out_ch)\n",
    "        return img  # (B, out_ch, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MHA_with_bias(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, attn_dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(embed_dim, num_heads, dropout=attn_dropout, batch_first=True)\n",
    "        # We'll manually add bias to attn weights via key_padding_mask-like approach using attn_mask arg.\n",
    "    def forward(self, x, attn_bias=None):\n",
    "        # x: (B, N, D)\n",
    "        B, N, D = x.shape\n",
    "        # attn_bias expected shape: (B, N, N) or None. MultiheadAttention accepts attn_mask of shape (N, N) or (B*num_heads, N, N) in latest versions.\n",
    "        # We'll collapse batch and apply per-sample attention via loop for clarity (small overhead).\n",
    "        outputs = []\n",
    "        for b in range(B):\n",
    "            xb = x[b:b+1]  # (1,N,D)\n",
    "            # attn_mask for nn.MultiheadAttention should be (N,N) where True/float(-inf) masks, but PyTorch expects float mask with -inf in positions to mask.\n",
    "            attn_mask_b = None\n",
    "            if attn_bias is not None:\n",
    "                # attn_bias[b]: (N,N) float where large negative values encourage zero attention.\n",
    "                attn_mask_b = attn_bias[b].to(x.device)  # float mask\n",
    "            out_b, _ = self.mha(xb, xb, xb, attn_mask=attn_mask_b)\n",
    "            outputs.append(out_b)\n",
    "        out = torch.cat(outputs, dim=0)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MHA_with_bias(embed_dim, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, int(embed_dim*mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(int(embed_dim*mlp_ratio), embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, attn_bias=None):\n",
    "        x = x + self.attn(self.norm1(x), attn_bias=attn_bias)\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class NoseTransformer(nn.Module):\n",
    "    def __init__(self, in_ch=4, out_ch=3, embed_dim=512, patch_size=8, depth=6, num_heads=8):\n",
    "        \"\"\"\n",
    "        in_ch: image channels + mask channel (e.g., 3+1)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.patch_embed = PatchEmbed(in_ch, embed_dim, patch_size)\n",
    "        self.pos_embed = None  # will init at forward on size\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(embed_dim, num_heads) for _ in range(depth)])\n",
    "        self.unembed = PatchUnembed(out_ch, patch_size, embed_dim)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, inp, patch_mask=None):\n",
    "        \"\"\"\n",
    "        inp: (B, in_ch, H, W)\n",
    "        patch_mask: (B, N) binary 0/1 indicating nose patches (optional)\n",
    "        \"\"\"\n",
    "        B = inp.shape[0]\n",
    "        x, (nh, nw) = self.patch_embed(inp)  # (B, N, D)\n",
    "        N = x.shape[1]\n",
    "        if self.pos_embed is None or self.pos_embed.shape[1] != N:\n",
    "            self.pos_embed = nn.Parameter(torch.zeros(1, N, self.embed_dim)).to(x.device)\n",
    "            nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        x = x + self.pos_embed\n",
    "\n",
    "        # Build attention bias if patch_mask given:\n",
    "        # We want queries from nose patches to preferentially attend to nose patches.\n",
    "        # Construct attn_bias of shape (B, N, N) where large negative value (-1e9) added\n",
    "        # for positions where query is nose and key is non-nose.\n",
    "        attn_bias = None\n",
    "        if patch_mask is not None:\n",
    "            # patch_mask: (B, N) 0/1\n",
    "            # Create base bias zeros\n",
    "            attn_bias = torch.zeros(B, N, N, device=x.device)\n",
    "            neg_inf = -1e9\n",
    "            for b in range(B):\n",
    "                pm = patch_mask[b].float()  # (N,)\n",
    "                # For queries where pm==1, keys where pm==0 -> add neg_inf\n",
    "                q_is_nose = pm.view(N, 1)  # (N,1)\n",
    "                k_is_nose = pm.view(1, N)  # (1,N)\n",
    "                mask_q_nose_key_not = (q_is_nose == 1) & (k_is_nose == 0)\n",
    "                attn_bias[b][mask_q_nose_key_not] = neg_inf\n",
    "            # nn.MultiheadAttention expects attn_mask of shape (N,N) with float values to add to attn logits.\n",
    "            # But because MultiheadAttention accepts attn_mask shared across batch, we will pass per-sample bias via our MHA_with_bias implementation.\n",
    "        # Pass through transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, attn_bias=attn_bias)\n",
    "\n",
    "        # decode\n",
    "        out = self.unembed(x, (nh,nw), out_ch=3)  # (B,3,H,W)\n",
    "        out = torch.sigmoid(out)  # outputs in 0..1\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self, layer_ids=[3, 8, 15, 22], weight=1.0):\n",
    "        \"\"\"\n",
    "        layer_ids: indices of VGG16 layers to extract features from.\n",
    "        weight: scaling factor for perceptual loss.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_FEATURES).features\n",
    "        self.selected_layers = layer_ids\n",
    "        self.weight = weight\n",
    "        self.vgg = vgg.eval()\n",
    "        for p in self.vgg.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # pred, target: (B, 3, H, W) normalized to ImageNet mean/std\n",
    "        loss = 0.0\n",
    "        x, y = pred, target\n",
    "        for i, layer in enumerate(self.vgg):\n",
    "            x = layer(x)\n",
    "            y = layer(y)\n",
    "            if i in self.selected_layers:\n",
    "                loss += nn.functional.l1_loss(x, y)\n",
    "        return self.weight * loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "imagenet_mean = torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1).to(device)\n",
    "imagenet_std = torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1).to(device)\n",
    "\n",
    "def preprocess_for_vgg(img):\n",
    "    # img in range [0,1]\n",
    "    return (img - imagenet_mean) / imagenet_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, opt, device, patch_size, lambda_global=0.1, lambda_percep=0.1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    perceptual_loss_fn = PerceptualLoss(weight=lambda_percep).to(device)\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"train\"):\n",
    "        inp = batch['input'].to(device)   # (B,4,H,W)\n",
    "        mask = batch['mask'].to(device)   # (B,1,H,W)\n",
    "        target = batch['target'].to(device) # (B,3,H,W)\n",
    "\n",
    "        # compute patch-level mask\n",
    "        patch_mask = mask_to_patch_mask(mask, patch_size)  # (B,N)\n",
    "\n",
    "        # forward\n",
    "        pred = model(inp, patch_mask)  # (B,3,H,W)\n",
    "\n",
    "        # --- existing losses ---\n",
    "        l1_mask = F.l1_loss(pred * mask, target * mask, reduction='sum') / (mask.sum() + 1e-6)\n",
    "        l1_global = F.l1_loss(pred, target, reduction='mean')\n",
    "\n",
    "        # --- perceptual loss (normalize first) ---\n",
    "        pred_vgg = preprocess_for_vgg(torch.clamp(pred, 0, 1))\n",
    "        target_vgg = preprocess_for_vgg(torch.clamp(target, 0, 1))\n",
    "        loss_percep = perceptual_loss_fn(pred_vgg, target_vgg)\n",
    "\n",
    "        # --- total loss ---\n",
    "        loss = l1_mask + lambda_global * l1_global + loss_percep\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def validate(model, dataloader, device, patch_size):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"val\"):\n",
    "            inp = batch['input'].to(device)\n",
    "            mask = batch['mask'].to(device)\n",
    "            target = batch['target'].to(device)\n",
    "\n",
    "            patch_mask = mask_to_patch_mask(mask, patch_size)\n",
    "            pred = model(inp, patch_mask)\n",
    "            l1_mask = F.l1_loss(pred * mask, target * mask, reduction='sum') / (mask.sum() + 1e-6)\n",
    "            total_loss += l1_mask.item()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def infer_image(model, image_pil, mask_pil, device=\"cpu\", size=256, patch_size=16):\n",
    "    \"\"\"\n",
    "    image_pil: PIL RGB image\n",
    "    mask_pil: PIL L mask (binary)\n",
    "    \"\"\"\n",
    "    transform_img = T.Compose([T.Resize((size,size)), T.ToTensor()])\n",
    "    transform_mask = T.Compose([T.Resize((size,size)), T.ToTensor()])\n",
    "    img = transform_img(image_pil)  # (3,H,W)\n",
    "    mask = transform_mask(mask_pil) # (1,H,W)\n",
    "    inp = torch.cat([img, mask], dim=0).unsqueeze(0).to(device)\n",
    "    patch_mask = mask_to_patch_mask(mask.unsqueeze(0), patch_size).to(device)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(inp, patch_mask)  # (1,3,H,W)\n",
    "    out_cpu = out.squeeze(0).cpu()\n",
    "    # composite: keep outside mask from original, inside mask from output\n",
    "    composite = out_cpu * mask + img * (1 - mask)\n",
    "    return composite.clamp(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def main_train(\n",
    "    img_dir, mask_dir, target_dir,\n",
    "    out_dir='checkpoints', epochs=20, batch_size=8, lr=2e-4,\n",
    "    size=256, patch_size=8, device=None\n",
    "):\n",
    "    device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    ds = NoseFolderDataset(img_dir, mask_dir, target_dir, size=size)\n",
    "    n = len(ds)\n",
    "    split = int(n * 0.9)\n",
    "    train_ds = torch.utils.data.Subset(ds, list(range(split)))\n",
    "    val_ds = torch.utils.data.Subset(ds, list(range(split, n)))\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    model = NoseTransformer(in_ch=4, out_ch=3, embed_dim=512, patch_size=patch_size, depth=6, num_heads=8).to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    best_val = 1e9\n",
    "    for epoch in range(1, epochs+1):\n",
    "        print(f\"Epoch {epoch}/{epochs}\")\n",
    "        train_loss = train_one_epoch(model, train_loader, opt, device, patch_size)\n",
    "        val_loss = validate(model, val_loader, device, patch_size)\n",
    "        print(f\"  train_loss: {train_loss:.6f}, val_masked_L1: {val_loss:.6f}\")\n",
    "        # save\n",
    "        torch.save(model.state_dict(), os.path.join(out_dir, f'model_epoch{epoch}.pth'))\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            torch.save(model.state_dict(), os.path.join(out_dir, f'model_best.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example:\n",
    "    # python nose_transformer.py\n",
    "    # Set your folders here\n",
    "    IMG_DIR = \"/kaggle/input/ilab-facial-data/facial_images/processed2/train/A\"\n",
    "    MASK_DIR = \"/kaggle/working/nose_mask\"\n",
    "    TARGET_DIR = \"/kaggle/input/ilab-facial-data/facial_images/processed2/train/B\"\n",
    "    main_train(IMG_DIR, MASK_DIR, TARGET_DIR, out_dir='ckpts', epochs=10, patch_size=8, batch_size=8, lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test_and_save(model, dataloader, device, save_dir=\"results\", size=256):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model.eval()\n",
    "\n",
    "    to_pil = T.ToPILImage()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            x = batch[\"input\"].to(device)\n",
    "            target = batch[\"target\"].to(device)\n",
    "\n",
    "            # forward\n",
    "            pred = model(x)\n",
    "\n",
    "            # detach for CPU\n",
    "            input_img = batch[\"input\"][:, :3, :, :].cpu()  # first 3 channels = original image\n",
    "            target_img = target.cpu()\n",
    "            pred_img = pred.cpu()\n",
    "\n",
    "            # filenames\n",
    "            input_file = batch[\"input_file\"][0]\n",
    "            target_file = batch[\"target_file\"][0]\n",
    "            input_fn = os.path.basename(input_file)\n",
    "            target_fn = os.path.basename(target_file)\n",
    "\n",
    "            # convert to PIL\n",
    "            input_pil = Image.open(input_file).convert(\"RGB\").resize((size, size))\n",
    "            target_pil = Image.open(target_file).convert(\"RGB\").resize((size, size))\n",
    "            pred_pil = to_pil(pred_img[0].cpu().clamp(0,1))\n",
    "\n",
    "            # combine into one wide image\n",
    "            w, h = input_pil.size\n",
    "            combined = Image.new(\"RGB\", (w*3, h))\n",
    "            combined.paste(input_pil, (0,0))\n",
    "            combined.paste(target_pil, (w,0))\n",
    "            combined.paste(pred_pil, (w*2,0))\n",
    "\n",
    "            # save with reference to original filename\n",
    "            save_name = f\"{os.path.splitext(input_fn)[0]}__{os.path.splitext(target_fn)[0]}.png\"\n",
    "            combined.save(os.path.join(save_dir, save_name))\n",
    "\n",
    "    print(f\"Saved test results to {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_img_dir = \"/kaggle/input/ilab-facial-data/facial_images/processed2/val/A\"\n",
    "test_mask_dir = \"/kaggle/working/test_nose_mask\"\n",
    "test_target_dir = \"/kaggle/input/ilab-facial-data/facial_images/processed2/val/B\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "\n",
    "test_ds = NoseFolderDataset(test_img_dir, test_mask_dir, test_target_dir, size=256)\n",
    "test_loader = DataLoader(test_ds, batch_size=4, shuffle=False)\n",
    "\n",
    "# load best model\n",
    "model = NoseTransformer(in_ch=4, out_ch=3, embed_dim=512, patch_size=8,\n",
    "                        depth=6, num_heads=8).to(device)\n",
    "model.load_state_dict(torch.load(\"/kaggle/working/ckpts/model_best.pth\", map_location=device))\n",
    "\n",
    "# run test\n",
    "test_and_save(model, test_loader, save_dir=\"test_results\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load image with OpenCV (BGR format)\n",
    "img = cv2.imread(\"/kaggle/working/test_results/pre_WhatsApp Image 2025-07-12 at 1.30.12 AM (1)__post_WhatsApp Image 2025-07-12 at 1.30.12 AM (1).png\")\n",
    "\n",
    "# Convert BGR → RGB (since OpenCV loads in BGR)\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Plot with matplotlib\n",
    "plt.imshow(img_rgb)\n",
    "plt.axis(\"off\")  # hide axis\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8073816,
     "sourceId": 12803539,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
