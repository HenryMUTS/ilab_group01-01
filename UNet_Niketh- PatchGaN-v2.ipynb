{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load pretrained landmark model (68 points, 2D)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_alignment\n",
    "fa = face_alignment.FaceAlignment(face_alignment.LandmarksType.TWO_D, device=device, flip_input=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nose_mask(image_path, save_path, log):\n",
    "    try:\n",
    "        img = cv2.imread(image_path)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "        preds = fa.get_landmarks(img_rgb)\n",
    "        if preds is None:\n",
    "            return False\n",
    "            \n",
    "    except Warning as w:  # Catch warnings as exceptions\n",
    "        return False\n",
    "\n",
    "    landmarks = preds[0]\n",
    "    nose_points = landmarks[27:36]  # indexes 27–35 in 0-based Python index\n",
    "\n",
    "    mask = np.zeros(img.shape[:2], dtype=np.uint8)\n",
    "    cv2.fillConvexPoly(mask, np.int32(nose_points), 255)\n",
    "\n",
    "    cv2.imwrite(save_path, mask)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "def batch_create_nose_masks(src_dir, dst_dir, create_func, exts=('.jpg','.jpeg','.png')):\n",
    "    \"\"\"\n",
    "    Generate nose masks for all images in src_dir and save in dst_dir.\n",
    "    Skips files if the mask already exists.\n",
    "    \"\"\"\n",
    "    os.makedirs(dst_dir, exist_ok=True)\n",
    "    warnings.filterwarnings(\"error\", message=\"No faces were detected.\")\n",
    "\n",
    "    for file in os.listdir(src_dir):\n",
    "        if file.lower().endswith(exts):\n",
    "            in_path  = os.path.join(src_dir, file)\n",
    "            out_path = os.path.join(dst_dir, file)\n",
    "            if not os.path.exists(out_path):\n",
    "                try:\n",
    "                    create_func(in_path, out_path, logging)\n",
    "                except Warning as w:\n",
    "                    logging.warning(f\"Skipped {file}: {w}\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error with {file}: {e}\")\n",
    "\n",
    "base_dir = \"/workspace/data_splits\"\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "for split in splits:\n",
    "    input_folder = os.path.join(base_dir, split, \"input\")\n",
    "    mask_folder  = os.path.join(base_dir, split, \"mask_input\")\n",
    "    batch_create_nose_masks(input_folder, mask_folder, create_nose_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_patches(x, patch_size):\n",
    "    # x: (B, C, H, W)\n",
    "    B, C, H, W = x.shape\n",
    "    assert H % patch_size == 0 and W % patch_size == 0\n",
    "    ph, pw = patch_size, patch_size\n",
    "    nh, nw = H // ph, W // pw\n",
    "    x = x.reshape(B, C, nh, ph, nw, pw)\n",
    "    x = x.permute(0,2,4,3,5,1).reshape(B, nh*nw, ph*pw*C)  # (B, N, patch_dim)\n",
    "    return x, (nh, nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patches_to_img(patches, patch_size, nh_nw, C):\n",
    "    # patches: (B, N, patch_dim)\n",
    "    B, N, D = patches.shape\n",
    "    nh, nw = nh_nw\n",
    "    ph = pw = patch_size\n",
    "    x = patches.reshape(B, nh, nw, ph, pw, C).permute(0,5,1,3,2,4).reshape(B, C, nh*ph, nw*pw)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_patch_mask(mask, patch_size):\n",
    "    # mask: (B,1,H,W) binary [0,1]\n",
    "    B, _, H, W = mask.shape\n",
    "    ph = pw = patch_size\n",
    "    nh, nw = H//ph, W//pw\n",
    "    mask = mask.reshape(B, 1, nh, ph, nw, pw)\n",
    "    mask = mask.mean(dim=(3,5))  # (B,1,nh,nw)\n",
    "    patch_mask = (mask.view(B, nh*nw) > 0.1).float()  # (B, N)\n",
    "    return patch_mask  # 1 where patch contains nos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\"}\n",
    "\n",
    "def _map_stem_to_name(dir_path: Path):\n",
    "    \"\"\"Return {stem: filename} for allowed image files (first match per stem).\"\"\"\n",
    "    m = {}\n",
    "    for p in dir_path.iterdir():\n",
    "        if p.is_file() and p.suffix.lower() in IMG_EXTS:\n",
    "            m.setdefault(p.stem, p.name)  # keep first occurrence\n",
    "    return m\n",
    "\n",
    "def validate_files(img_dir, mask_dir, target_dir, verbose=True):\n",
    "    img_dir, mask_dir, target_dir = map(Path, (img_dir, mask_dir, target_dir))\n",
    "\n",
    "    img_map    = _map_stem_to_name(img_dir)\n",
    "    mask_map   = _map_stem_to_name(mask_dir)\n",
    "    target_map = _map_stem_to_name(target_dir)\n",
    "\n",
    "    img_stems    = set(img_map.keys())\n",
    "    mask_stems   = set(mask_map.keys())\n",
    "    target_stems = set(target_map.keys())\n",
    "\n",
    "    common = sorted(img_stems & mask_stems & target_stems)\n",
    "\n",
    "    missing_mask   = sorted(img_stems - mask_stems)\n",
    "    missing_target = sorted(img_stems - target_stems)\n",
    "    orphan_masks   = sorted(mask_stems - img_stems)\n",
    "    orphan_targets = sorted(target_stems - img_stems)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[validate] counts: img={len(img_stems)} mask={len(mask_stems)} target={len(target_stems)}\")\n",
    "        print(f\"[validate] common triples: {len(common)}\")\n",
    "        if missing_mask:   print(f\"[validate] missing masks for {len(missing_mask)} imgs, e.g. {missing_mask[:5]}\")\n",
    "        if missing_target: print(f\"[validate] missing targets for {len(missing_target)} imgs, e.g. {missing_target[:5]}\")\n",
    "        if orphan_masks:   print(f\"[validate] masks without imgs: {len(orphan_masks)}, e.g. {orphan_masks[:5]}\")\n",
    "        if orphan_targets: print(f\"[validate] targets without imgs: {len(orphan_targets)}, e.g. {orphan_targets[:5]}\")\n",
    "\n",
    "    # Build (img_filename, target_filename) using the exact filenames found\n",
    "    pairs = [(img_map[s], target_map[s]) for s in common]\n",
    "\n",
    "    if len(pairs) == 0:\n",
    "        raise ValueError(\n",
    "            \"No valid (img, mask, target) triples found.\\n\"\n",
    "            f\"Checked:\\n  img_dir={img_dir}\\n  mask_dir={mask_dir}\\n  target_dir={target_dir}\\n\"\n",
    "            \"See [validate] logs above for mismatches.\"\n",
    "        )\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def collate_keep_aspect(batch, multiple=32):\n",
    "    \"\"\"\n",
    "    Batch collate that keeps aspect ratios.\n",
    "    Pads all images/masks/targets in the batch to the max H and W,\n",
    "    rounded up to the nearest multiple.\n",
    "    \"\"\"\n",
    "    imgs, tgts, masks, orig_hw, files = [], [], [], [], []\n",
    "\n",
    "    for b in batch:\n",
    "        x = b[\"input\"]    # [3,H,W]\n",
    "        y = b[\"target\"]   # [3,H,W]\n",
    "        m = b[\"mask\"]     # [1,H,W]\n",
    "        H, W = x.shape[1:]\n",
    "\n",
    "        imgs.append(x)\n",
    "        tgts.append(y)\n",
    "        masks.append(m)\n",
    "        orig_hw.append(torch.tensor([H, W], dtype=torch.int32))\n",
    "        files.append((b.get(\"input_file\",\"\"), b.get(\"mask_file\",\"\"), b.get(\"target_file\",\"\")))\n",
    "\n",
    "    # find max height/width in this batch\n",
    "    Ht = max(t.shape[1] for t in imgs)\n",
    "    Wt = max(t.shape[2] for t in imgs)\n",
    "\n",
    "    # round up to nearest multiple (stride)\n",
    "    Ht = (Ht + multiple - 1) // multiple * multiple\n",
    "    Wt = (Wt + multiple - 1) // multiple * multiple\n",
    "\n",
    "    def pad_to(t, Ht, Wt):\n",
    "        # pad as (left, right, top, bottom)\n",
    "        return F.pad(t, (0, Wt - t.shape[2], 0, Ht - t.shape[1]))\n",
    "\n",
    "    X = torch.stack([pad_to(t, Ht, Wt) for t in imgs])\n",
    "    Y = torch.stack([pad_to(t, Ht, Wt) for t in tgts])\n",
    "    M = torch.stack([pad_to(t, Ht, Wt) for t in masks])\n",
    "    OHW = torch.stack(orig_hw)\n",
    "\n",
    "    return {\n",
    "        \"input\": X,     # [B,3,Ht,Wt]\n",
    "        \"target\": Y,    # [B,3,Ht,Wt]\n",
    "        \"mask\": M,      # [B,1,Ht,Wt]\n",
    "        \"orig_hw\": OHW, # [B,2]\n",
    "        \"files\": files\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "import torch, cv2\n",
    "import numpy as np\n",
    "\n",
    "class NoseFolderDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, target_dir, size=256):\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.mask_dir = Path(mask_dir)\n",
    "        self.target_dir = Path(target_dir)\n",
    "        self.file_pairs = validate_files(self.img_dir, self.mask_dir, self.target_dir)   # [(img_name, tgt_name), ...]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_pairs)\n",
    "\n",
    "    def _read_rgb(self, path):\n",
    "        img = cv2.imread(str(path), cv2.IMREAD_COLOR)\n",
    "        if img is None:\n",
    "            raise FileNotFoundError(path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)        # (H,W,3) uint8\n",
    "        return img\n",
    "\n",
    "    def _read_mask(self, path):\n",
    "        \n",
    "        m = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)\n",
    "        if m is None:\n",
    "            raise FileNotFoundError(path)\n",
    "        m = (m > 127).astype(np.uint8)                    # (H,W) 0/1\n",
    "        return m\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname, target_f = self.file_pairs[idx]\n",
    "        ip, mp, tp = self.img_dir/fname, self.mask_dir/fname, self.target_dir/target_f\n",
    "\n",
    "        img    = self._read_rgb(ip)     # H,W,3\n",
    "        target = self._read_rgb(tp)     # H,W,3\n",
    "        mask   = self._read_mask(mp)    # H,W\n",
    "\n",
    "        if img.shape[:2] != target.shape[:2]:\n",
    "            # minimal: resize target to img (or vice-versa, pick one consistently)\n",
    "            target = cv2.resize(target, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_AREA)\n",
    "        if img.shape[:2] != mask.shape[:2]:\n",
    "            mask = cv2.resize(mask, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        H, W = img.shape[:2]\n",
    "        img_t  = torch.from_numpy(img).permute(2,0,1).float()/255.0\n",
    "        tgt_t  = torch.from_numpy(target).permute(2,0,1).float()/255.0\n",
    "        mask_t = torch.from_numpy(mask).unsqueeze(0).float()\n",
    "\n",
    "        return {\n",
    "            \"input\": img_t, \"target\": tgt_t, \"mask\": mask_t,\n",
    "            \"orig_h\": H, \"orig_w\": W,\n",
    "            \"input_file\": str(ip), \"mask_file\": str(mp), \"target_file\": str(tp),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- Generator (UnE) ----------\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ----- tiny mask helpers\n",
    "def dilate_mask_binary(mask, k=11, iters=2):\n",
    "    out = mask\n",
    "    for _ in range(iters):\n",
    "        out = F.max_pool2d(out, kernel_size=k, stride=1, padding=k//2)\n",
    "    return out.clamp(0,1)\n",
    "\n",
    "def feather_mask(mask, k=9):\n",
    "    return F.avg_pool2d(mask, kernel_size=k, stride=1, padding=k//2).clamp(0,1)\n",
    "\n",
    "\n",
    "# --------------------------- UNet blocks ---------------------------\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(out_ch, affine=True),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(out_ch, affine=True),\n",
    "            nn.SiLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x): return self.block(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.conv = ConvBlock(in_ch, out_ch)\n",
    "    def forward(self, x): return self.conv(self.pool(x))\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.up   = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        self.conv = ConvBlock(in_ch, out_ch)\n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        # pad if needed (in case odd dims)\n",
    "        diffY = skip.size(2) - x.size(2)\n",
    "        diffX = skip.size(3) - x.size(3)\n",
    "        if diffY != 0 or diffX != 0:\n",
    "            x = F.pad(x, (0, diffX, 0, diffY))\n",
    "        x = torch.cat([skip, x], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "# --------------------------- UNet Generator (residual) ---------------------------\n",
    "class UNetNoseGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    UNet that takes RGB+mask (4ch) and predicts a 3ch residual 'delta'.\n",
    "    Output = rgb + soft_mask * tanh(raw) * scale\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch=4, out_ch=3, base=64, depth=5, res_max=0.75):\n",
    "        \"\"\"\n",
    "        depth=5 -> downsample x2 five times (stride 32). Use collate multiple=32.\n",
    "        For less memory, set depth=4 (stride 16).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.out_ch = out_ch\n",
    "        self.res_max = res_max\n",
    "\n",
    "        # encoder\n",
    "        self.inc  = ConvBlock(in_ch, base)                 # H\n",
    "        self.down1 = Down(base,     base*2)                # H/2\n",
    "        self.down2 = Down(base*2,   base*4)                # H/4\n",
    "        self.down3 = Down(base*4,   base*8)                # H/8\n",
    "        self.down4 = Down(base*8,   base*8)                # H/16\n",
    "        self.has_down5 = (depth >= 5)\n",
    "        if self.has_down5:\n",
    "            self.down5 = Down(base*8, base*8)              # H/32\n",
    "\n",
    "        # decoder\n",
    "        if self.has_down5:\n",
    "            self.up1 = Up(base*8 + base*8, base*8)         # concat with down4\n",
    "            ch_up_in = base*8 + base*8\n",
    "        else:\n",
    "            # if no down5, first up will concatenate down3 and bottleneck at base*8\n",
    "            ch_up_in = base*8 + base*8  # consistent with next lines\n",
    "\n",
    "        self.up2 = Up(base*8 + base*8, base*8)             # + down3\n",
    "        self.up3 = Up(base*8 + base*4, base*4)             # + down2\n",
    "        self.up4 = Up(base*4 + base*2, base*2)             # + down1\n",
    "        self.up5 = Up(base*2 + base,   base)               # + inc\n",
    "\n",
    "        self.outc = nn.Conv2d(base, out_ch, kernel_size=3, padding=1)\n",
    "        nn.init.zeros_(self.outc.weight)\n",
    "        nn.init.zeros_(self.outc.bias)\n",
    "\n",
    "        # learnable residual cap\n",
    "        self._alpha = nn.Parameter(torch.tensor(0.0))  # sigmoid ~0.5 initially\n",
    "\n",
    "    def forward(self, inp, return_full=False):\n",
    "        \"\"\"\n",
    "        inp: [B,4,H,W]  (RGB + binary/soft mask in channel 4)\n",
    "        returns: [B,3,H,W] blended output at original size\n",
    "        \"\"\"\n",
    "        rgb   = inp[:, :3]\n",
    "        mask1 = inp[:, 3:4]\n",
    "\n",
    "        # encoder\n",
    "        x1 = self.inc(inp)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        if self.has_down5:\n",
    "            x6 = self.down5(x5)\n",
    "            u1 = self.up1(x6, x5)\n",
    "        else:\n",
    "            u1 = self.up2(x5, x4)  # skip one level if depth=4\n",
    "\n",
    "        # decoder path\n",
    "        if self.has_down5:\n",
    "            u2 = self.up2(u1, x4)\n",
    "        else:\n",
    "            u2 = u1\n",
    "        u3 = self.up3(u2, x3)\n",
    "        u4 = self.up4(u3, x2)\n",
    "        u5 = self.up5(u4, x1)\n",
    "\n",
    "        raw   = self.outc(u5)               # [B,3,H,W]\n",
    "        scale = torch.sigmoid(self._alpha) * self.res_max\n",
    "        delta = torch.tanh(raw) * scale     # bounded residual\n",
    "\n",
    "        # soft blend within a feathered mask band\n",
    "        hard   = dilate_mask_binary(mask1, k=11, iters=2)\n",
    "        m_soft = feather_mask(hard, k=9)            # [B,1,H,W]\n",
    "        m3     = m_soft.repeat(1, 3, 1, 1)\n",
    "        full_rgb = (rgb + delta)\n",
    "        out    = rgb + delta * m3\n",
    "        if return_full:\n",
    "            return out, full_rgb, m_soft\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "class VGGPerceptualLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    VGG16 feature L1 loss. Works on inputs in [0,1].\n",
    "    If mask is given (B,1,H,W), it is downsampled per VGG stage and used\n",
    "    to weight the feature differences spatially.\n",
    "    \"\"\"\n",
    "    def __init__(self, layers=(3, 8, 15, 22), layer_weights=None):\n",
    "        super().__init__()\n",
    "        # relu1_2=3, relu2_2=8, relu3_3=15, relu4_3=22 in torchvision VGG16.features\n",
    "        self.layers = tuple(layers)\n",
    "        self.layer_weights = (\n",
    "            [1.0] * len(self.layers) if layer_weights is None else layer_weights\n",
    "        )\n",
    "\n",
    "        vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_FEATURES).features\n",
    "        self.vgg = vgg.eval()\n",
    "        for p in self.vgg.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "        # register mean/std buffers for ImageNet normalization\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1)\n",
    "        std  = torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1)\n",
    "        self.register_buffer('mean', mean, persistent=False)\n",
    "        self.register_buffer('std',  std,  persistent=False)\n",
    "\n",
    "    def _norm(self, x):\n",
    "        # x expected in [0,1]\n",
    "        return (x - self.mean) / self.std\n",
    "\n",
    "    def forward(self, pred, target, mask=None):\n",
    "        \"\"\"\n",
    "        pred, target: (B,3,H,W) in [0,1]\n",
    "        mask (optional): (B,1,H,W) with 0..1 weights (e.g., your nose mask)\n",
    "        \"\"\"\n",
    "        x = self._norm(pred)\n",
    "        y = self._norm(target)\n",
    "\n",
    "        loss = 0.0\n",
    "        idx_set = set(self.layers)\n",
    "        lw_iter = iter(self.layer_weights)\n",
    "\n",
    "        # run through the VGG and collect selected layers\n",
    "        for i, layer in enumerate(self.vgg):\n",
    "            x = layer(x)\n",
    "            y = layer(y)\n",
    "\n",
    "            if i in idx_set:\n",
    "                w = next(lw_iter)\n",
    "                if mask is None:\n",
    "                    # plain feature L1\n",
    "                    loss += w * F.l1_loss(x, y)\n",
    "                else:\n",
    "                    # masked feature L1 (downsample mask to feature size)\n",
    "                    m = F.interpolate(mask, size=x.shape[-2:], mode='bilinear', align_corners=False)\n",
    "                    num = (m * (x - y).abs()).sum()\n",
    "                    den = m.sum() + 1e-6\n",
    "                    loss += w * (num / den)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilate_mask_binary(mask, k=11, iters=2):\n",
    "    \"\"\"Binary dilation. mask: (B,1,H,W) in {0,1} float.\"\"\"\n",
    "    out = mask\n",
    "    for _ in range(iters):\n",
    "        out = F.max_pool2d(out, kernel_size=k, stride=1, padding=k//2)\n",
    "    return out.clamp(0,1)\n",
    "\n",
    "def erode_mask_binary(mask, k=11, iters=1):\n",
    "    \"\"\"Binary erosion via max-pool trick.\"\"\"\n",
    "    x = 1.0 - mask\n",
    "    for _ in range(iters):\n",
    "        x = F.max_pool2d(x, kernel_size=k, stride=1, padding=k//2)\n",
    "    return (1.0 - x).clamp(0,1)\n",
    "\n",
    "def feather_mask(mask, k=9):\n",
    "    \"\"\"Soft edge (0..1) via avg-pool (feathered band).\"\"\"\n",
    "    return F.avg_pool2d(mask, kernel_size=k, stride=1, padding=k//2).clamp(0,1)\n",
    "\n",
    "# ---------- patch mask helper ----------\n",
    "def mask_to_patch_mask(mask, patch_size):\n",
    "    \"\"\"\n",
    "    Convert (B,1,H,W) 0/1 (or soft) mask to (B, N) patch weights by\n",
    "    averaging within non-overlapping p×p patches.\n",
    "    \"\"\"\n",
    "    B, _, H, W = mask.shape\n",
    "    p = patch_size\n",
    "    # trim if H/W not divisible by p\n",
    "    Ht, Wt = (H // p) * p, (W // p) * p\n",
    "    m = mask[:, :, :Ht, :Wt]\n",
    "    # (B,1, H/p, p, W/p, p) -> mean over the two 'p' dims -> (B,1,H/p,W/p)\n",
    "    m = m.view(B, 1, Ht//p, p, Wt//p, p).mean(dim=(3,5))\n",
    "    return m.flatten(1)  # (B, N)\n",
    "\n",
    "# ---------- simple PatchGAN (1-channel real/fake map) ----------\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, in_ch=6, base=64):\n",
    "        super().__init__()\n",
    "        c = base\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch,   c, 4, 2, 1), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(c,     c*2, 4, 2, 1), nn.BatchNorm2d(c*2), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(c*2,   c*4, 4, 2, 1), nn.BatchNorm2d(c*4), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(c*4,   c*8, 4, 2, 1), nn.BatchNorm2d(c*8), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(c*8,     1, 4, 1, 1)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "        \n",
    "\n",
    "# ---------- validation (masked L1 using the same soft mask) ----------\n",
    "@torch.no_grad()\n",
    "def validate_epoch(G, loader, device):\n",
    "    G.eval()\n",
    "    tot, n = 0.0, 0\n",
    "    for batch in loader:\n",
    "        rgb    = batch[\"input\"].to(device)   # (B,4,H,W)  [RGB+mask]\n",
    "        target = batch[\"target\"].to(device)  # (B,3,H,W)\n",
    "        mask   = batch[\"mask\"].to(device)    # (B,1,H,W)\n",
    "        inp = torch.cat([rgb, mask], dim=1)\n",
    "        # same expansion/feathering as train\n",
    "        hard   = dilate_mask_binary(mask, k=11, iters=2)\n",
    "        m_soft = feather_mask(hard, k=9)\n",
    "\n",
    "        pred = G(inp)\n",
    "\n",
    "        l1m = ((pred - target).abs() * m_soft).sum() / (m_soft.sum() + 1e-6)\n",
    "        b = inp.size(0)\n",
    "        tot += l1m.item() * b\n",
    "        n   += b\n",
    "    return tot / max(1,n)\n",
    "\n",
    "# ---------- training ----------\n",
    "def train_gan(\n",
    "    train_loader, val_loader, *,\n",
    "    epochs=20, out_dir=\"ckpts_gan\",\n",
    "    lr_G=2e-4, lr_D=1e-4,\n",
    "    adv_start_w=0.05,\n",
    "    lambda_l1=5.0, lambda_out_id=0.5,\n",
    "    use_perc=False, perceptual_fn=None,\n",
    "    amp=True, device=\"cuda\",\n",
    "    resume_from=None                 # <--- NEW\n",
    "):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # ----- build models -----\n",
    "    G = UNetNoseGenerator(in_ch=4, out_ch=3, base=64, depth=5, res_max=0.75).to(device)\n",
    "    D = PatchDiscriminator(in_ch=6, base=64).to(device)\n",
    "\n",
    "    # ----- opt -----\n",
    "    opt_G = torch.optim.AdamW(G.parameters(), lr=lr_G, betas=(0.5, 0.999))\n",
    "    opt_D = torch.optim.AdamW(D.parameters(), lr=lr_D, betas=(0.5, 0.999))\n",
    "\n",
    "    # ----- (optional) resume -----\n",
    "    start_ep = 1\n",
    "    best = float(\"inf\")\n",
    "    if resume_from and os.path.isfile(resume_from):\n",
    "        ckpt = torch.load(resume_from, map_location=device)\n",
    "\n",
    "        # load weights (strict=True recommended if same arch; relax to False if needed)\n",
    "        G.load_state_dict(ckpt[\"G\"], strict=True)\n",
    "        D.load_state_dict(ckpt[\"D\"], strict=True)\n",
    "\n",
    "        # load optimizers\n",
    "        if \"opt_G\" in ckpt: opt_G.load_state_dict(ckpt[\"opt_G\"])\n",
    "        if \"opt_D\" in ckpt: opt_D.load_state_dict(ckpt[\"opt_D\"])\n",
    "\n",
    "        # resume epoch/best\n",
    "        start_ep = int(ckpt.get(\"epoch\", 0)) + 1\n",
    "        best = float(ckpt.get(\"val_l1m\", best))\n",
    "        print(f\"[resume] from {resume_from} → start_ep={start_ep}  best={best:.4f}\")\n",
    "    else:\n",
    "        if resume_from:\n",
    "            print(f\"[resume] file not found: {resume_from} (starting fresh)\")\n",
    "\n",
    "    # ----- schedulers (recreated; positioned via last_epoch) -----\n",
    "    # We stepped schedulers once per epoch at the END, so set last_epoch=start_ep-2\n",
    "    # so that after the first loop’s step(), they land on epoch (start_ep-1)→start_ep properly.\n",
    "    sch_G = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        opt_G, T_max=epochs, eta_min=lr_G*0.1, last_epoch=start_ep-2\n",
    "    )\n",
    "    sch_D = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        opt_D, T_max=epochs, eta_min=lr_D*0.1, last_epoch=start_ep-2\n",
    "    )\n",
    "\n",
    "    scaler_G = torch.amp.GradScaler('cuda', enabled=amp)\n",
    "    scaler_D = torch.amp.GradScaler('cuda', enabled=amp)\n",
    "\n",
    "    best_path = None\n",
    "\n",
    "    for ep in range(start_ep, epochs+1):\n",
    "        G.train(); D.train()\n",
    "        pbar = tqdm(train_loader, desc=f\"train {ep}/{epochs}\")\n",
    "        adv_w = adv_start_w * min(1.0, ep/5.0)\n",
    "\n",
    "        for batch in pbar:\n",
    "            rgb    = batch[\"input\"].to(device)   # (B,3,H,W)\n",
    "            target = batch[\"target\"].to(device)  # (B,3,H,W)\n",
    "            mask   = batch[\"mask\"].to(device)    # (B,1,H,W)\n",
    "            inp    = torch.cat([rgb, mask], dim=1)  # (B,4,H,W)\n",
    "\n",
    "            hard   = dilate_mask_binary(mask, k=11, iters=2)\n",
    "            m_soft = feather_mask(hard, k=9)\n",
    "            outside = 1.0 - erode_mask_binary(hard, k=11, iters=2)\n",
    "\n",
    "            # ---- D step ----\n",
    "            opt_D.zero_grad(set_to_none=True)\n",
    "            with torch.amp.autocast('cuda', enabled=amp):\n",
    "                with torch.no_grad():\n",
    "                    fake = G(inp)\n",
    "                real_logits = D(torch.cat([rgb, target], dim=1))\n",
    "                fake_logits = D(torch.cat([rgb, fake],   dim=1))\n",
    "                d_loss = torch.relu(1. - real_logits).mean() + torch.relu(1. + fake_logits).mean()\n",
    "            scaler_D.scale(d_loss).backward()\n",
    "            scaler_D.step(opt_D)\n",
    "            scaler_D.update()\n",
    "\n",
    "            # ---- G step ----\n",
    "            opt_G.zero_grad(set_to_none=True)\n",
    "            with torch.amp.autocast('cuda', enabled=amp):\n",
    "                pred = G(inp)\n",
    "                l1_mask  = ((pred - target).abs() * m_soft).sum() / (m_soft.sum() + 1e-6)\n",
    "                l_out_id = ((pred - rgb).abs()    * outside).sum() / (outside.sum() + 1e-6)\n",
    "                fake_logits_g = D(torch.cat([rgb, pred], dim=1))\n",
    "                adv_loss = -fake_logits_g.mean()\n",
    "                if use_perc and (perceptual_fn is not None):\n",
    "                    perc_loss = perceptual_fn(pred, target, mask=m_soft)\n",
    "                else:\n",
    "                    perc_loss = 0.0\n",
    "                g_loss = (lambda_l1 * l1_mask\n",
    "                          + lambda_out_id * l_out_id\n",
    "                          + adv_w * adv_loss\n",
    "                          + 0.2 * perc_loss)\n",
    "            scaler_G.scale(g_loss).backward()\n",
    "            scaler_G.step(opt_G)\n",
    "            scaler_G.update()\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"D\": f\"{d_loss.item():.3f}\",\n",
    "                \"G\": f\"{g_loss.item():.3f}\",\n",
    "                \"L1m\": f\"{l1_mask.item():.4f}\",\n",
    "                \"OID\": f\"{l_out_id.item():.4f}\",\n",
    "                \"ADV\": f\"{adv_loss.item():.4f}\",\n",
    "                \"lrG\": f\"{sch_G.get_last_lr()[0]:.2e}\"\n",
    "            })\n",
    "\n",
    "        sch_G.step(); sch_D.step()\n",
    "\n",
    "        # ---- validation ----\n",
    "        val_l1m = validate_epoch(G, val_loader, device)\n",
    "\n",
    "        # ---- save ----\n",
    "        ckpt = {\n",
    "            \"epoch\": ep,\n",
    "            \"G\": G.state_dict(),\n",
    "            \"D\": D.state_dict(),\n",
    "            \"opt_G\": opt_G.state_dict(),\n",
    "            \"opt_D\": opt_D.state_dict(),\n",
    "            \"val_l1m\": val_l1m,\n",
    "            \"cfg\": {\n",
    "                \"lambda_l1\": lambda_l1,\n",
    "                \"lambda_out_id\": lambda_out_id,\n",
    "                \"adv_start_w\": adv_start_w\n",
    "            }\n",
    "            # (Optional) you can also save sched/scaler state next time:\n",
    "            # \"sch_G\": sch_G.state_dict(), \"sch_D\": sch_D.state_dict(),\n",
    "            # \"scaler_G\": scaler_G.state_dict(), \"scaler_D\": scaler_D.state_dict(),\n",
    "        }\n",
    "        last_path = os.path.join(out_dir, \"last.pt\")\n",
    "        torch.save(ckpt, last_path)\n",
    "\n",
    "        if val_l1m < best:\n",
    "            best = val_l1m\n",
    "            best_path = os.path.join(out_dir, \"best_l1_mask.pt\")\n",
    "            torch.save(ckpt, best_path)\n",
    "            saved_best = True\n",
    "        else:\n",
    "            saved_best = False\n",
    "\n",
    "        print(f\"[epoch {ep}] val L1(mask)={val_l1m:.4f} | best={best:.4f} | saved_best={saved_best}\")\n",
    "\n",
    "    return best_path or last_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"autocast is:\", getattr(globals(), \"autocast\", \"<no alias>\"))\n",
    "print(\"torch.autocast:\", torch.autocast)\n",
    "print(\"GradScaler:\", torch.amp.GradScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Use the keep-size dataset (returns RGB (3ch) + mask (1ch) separately)\n",
    "train_ds = NoseFolderDataset(\n",
    "    \"/workspace/data_splits/train/input\",\n",
    "    \"/workspace/data_splits/train/mask_input\",\n",
    "    \"/workspace/data_splits/train/target\"\n",
    ")\n",
    "val_ds = NoseFolderDataset(\n",
    "    \"/workspace/data_splits/val/input\",\n",
    "    \"/workspace/data_splits/val/mask_input\",\n",
    "    \"/workspace/data_splits/val/target\"\n",
    ")\n",
    "\n",
    "MULT = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=2, shuffle=True, num_workers=2, pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "    collate_fn=lambda b: collate_keep_aspect(b, multiple=MULT)   # no token cap needed\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=2, shuffle=False, num_workers=2, pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "    collate_fn=lambda b: collate_keep_aspect(b, multiple=MULT)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    TRAIN_IMG = \"/workspace/data_splits/train/input\"\n",
    "    TRAIN_MSK = \"/workspace/data_splits/mask_input\"\n",
    "    TRAIN_TGT = \"/workspace/data_splits/train/target\"\n",
    "\n",
    "    VAL_IMG   = \"/workspace/data_splits/val/input\"\n",
    "    VAL_MSK   = \"/workspace/data_splits/val/mask_input\"\n",
    "    VAL_TGT   = \"/workspace/data_splits/val/target\"\n",
    "\n",
    "    size = 256\n",
    "    patch_size = 8\n",
    "    bs = 8\n",
    "\n",
    "    perceptual_fn = VGGPerceptualLoss(layers=(3,8,15,22)).to(device)\n",
    "    \n",
    "    resume_ckpt = \"ckpts_Unet_PatchGan_Resv1/last.pt\"\n",
    "\n",
    "    best_path = train_gan(\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epochs=40,                     # total epochs you want to run\n",
    "    out_dir=\"ckpts_Unet_PatchGan_Resv1\",\n",
    "    amp=True,\n",
    "    device=device,\n",
    "    resume_from=resume_ckpt        # <-- resume here\n",
    "    )\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# ---- helpers: torch <-> numpy, mask prep ----\n",
    "def _torch_rgb_to_u8_bgr(x: torch.Tensor) -> np.ndarray:\n",
    "    # x: [3,H,W] float in [0,1]\n",
    "    x = (x.clamp(0,1) * 255.0).round().byte().cpu().numpy()       # CHW uint8\n",
    "    x = np.transpose(x, (1,2,0))                                   # HWC\n",
    "    return cv2.cvtColor(x, cv2.COLOR_RGB2BGR)                      # HWC BGR uint8\n",
    "\n",
    "def _bgr_u8_to_torch_rgb(x: np.ndarray) -> torch.Tensor:\n",
    "    # x: HWC BGR uint8\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = torch.from_numpy(np.transpose(x, (2,0,1))).float() / 255.0 # CHW float\n",
    "    return x\n",
    "\n",
    "def _mask_to_u8(mask_01: torch.Tensor, thresh=0.05, dilate=3):\n",
    "    # mask_01: [1,H,W] float in [0,1], soft or hard\n",
    "    m = (mask_01.squeeze(0).clamp(0,1) > thresh).byte().cpu().numpy() * 255  # H,W uint8 {0,255}\n",
    "    if dilate and dilate > 0:\n",
    "        k = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2*dilate+1, 2*dilate+1))\n",
    "        m = cv2.dilate(m, k)\n",
    "    return m  # H,W uint8\n",
    "\n",
    "def _mask_center(mask_u8: np.ndarray):\n",
    "    # center for seamlessClone\n",
    "    M = cv2.moments(mask_u8)\n",
    "    if M[\"m00\"] > 0:\n",
    "        cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "        cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "    else:\n",
    "        h, w = mask_u8.shape[:2]\n",
    "        cy, cx = h//2, w//2\n",
    "    return (cx, cy)\n",
    "\n",
    "# ---- main: Poisson blend one sample ----\n",
    "def poisson_blend_sample(pred_rgb, base_rgb, mask_01, mode=\"normal\", dilate=2, thresh=0.3):\n",
    "    src = _torch_rgb_to_u8_bgr(pred_rgb)   # full repaint\n",
    "    dst = _torch_rgb_to_u8_bgr(base_rgb)   # original\n",
    "    msk = _mask_to_u8(mask_01, thresh=thresh, dilate=dilate)  # ~binary 0/255\n",
    "    center = _mask_center(msk)\n",
    "    flag = cv2.NORMAL_CLONE if mode==\"normal\" else cv2.MIXED_CLONE\n",
    "    out = cv2.seamlessClone(src, dst, msk, center, flag)\n",
    "    return _bgr_u8_to_torch_rgb(out).to(pred_rgb.device)\n",
    "\n",
    "\n",
    "def save_triplet_fullsize(rgb_i, pred_i, tgt_i, save_dir, stem):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_image(rgb_i.clamp(0,1),  os.path.join(save_dir, f\"{stem}_input.png\"))\n",
    "    save_image(pred_i.clamp(0,1), os.path.join(save_dir, f\"{stem}_pred.png\"))\n",
    "    save_image(tgt_i.clamp(0,1),  os.path.join(save_dir, f\"{stem}_target.png\"))\n",
    "\n",
    "def pad_to_square(img, value=0.0):  # img: [3,H,W]\n",
    "    _, H, W = img.shape\n",
    "    S = max(H, W)\n",
    "    pad_t = (S - H)//2; pad_b = S - H - pad_t\n",
    "    pad_l = (S - W)//2; pad_r = S - W - pad_l\n",
    "    return F.pad(img, (pad_l, pad_r, pad_t, pad_b), value=value)  # [3,S,S]\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_test_unet(\n",
    "    ckpt_path,\n",
    "    test_loader,\n",
    "    device=\"cuda\",\n",
    "    save_dir=\"results_unet\",\n",
    "    panel_mode=\"none\",          # \"none\" or \"letterbox\"\n",
    "    panel_size=512,             # used only when panel_mode=\"letterbox\"\n",
    "    amp=True\n",
    "):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Build UNet exactly like in training\n",
    "    G = UNetNoseGenerator(in_ch=4, out_ch=3, base=64, depth=5, res_max=0.75).to(device)\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    G.load_state_dict(ckpt[\"G\"], strict=True)\n",
    "    G.eval()\n",
    "\n",
    "    idx = 0\n",
    "    autocast_ctx = torch.amp.autocast('cuda', enabled=amp)\n",
    "\n",
    "    for batch in test_loader:\n",
    "        rgb  = batch[\"input\"].to(device)    # [B,3,Ht,Wt] (padded)\n",
    "        mask = batch[\"mask\"].to(device)     # [B,1,Ht,Wt]\n",
    "        tgt  = batch[\"target\"].to(device)   # [B,3,Ht,Wt]\n",
    "        ohw  = batch.get(\"orig_hw\", None)   # [B,2] if using keep-aspect collate\n",
    "\n",
    "        inp = torch.cat([rgb, mask], dim=1) # [B,4,Ht,Wt]\n",
    "\n",
    "        with autocast_ctx:\n",
    "             out_masked, full_rgb, m_soft = G(inp, return_full=True)                   # [B,3,Ht,Wt]\n",
    "\n",
    "        B = full_rgb.size(0)\n",
    "        for i in range(B):\n",
    "            # crop back to each sample's original H,W (or current if not provided)\n",
    "            if ohw is not None:\n",
    "                H, W = int(ohw[i,0]), int(ohw[i,1])\n",
    "            else:\n",
    "                _, H, W = rgb[i].shape\n",
    "            rgb_i         = rgb[i, :, :H, :W]\n",
    "            tgt_i         = tgt[i,  :, :H, :W]\n",
    "            pred_masked_i = out_masked[i, :, :H, :W]\n",
    "            pred_full_i = full_rgb[i, :, :H, :W]\n",
    "            soft_i        = m_soft[i, :, :H, :W]   # absolute repaint (for Poisson)\n",
    "            '''  # masked blend (optional)\n",
    "            pred_i = pred[i, :, :H, :W]\n",
    "            tgt_i  = tgt[i,  :, :H, :W]\n",
    "            rgb_i  = rgb[i,  :, :H, :W]\n",
    "            mask_i = mask[i, :, :H, :W]\n",
    "\n",
    "            hard   = dilate_mask_binary(mask_i.unsqueeze(0), k=21, iters=3).squeeze(0) \n",
    "            soft   = feather_mask(hard.unsqueeze(0), k=15).squeeze(0)\n",
    "            '''\n",
    "            #blended_i = poisson_blend_sample(pred_rgb=pred_i, base_rgb=rgb_i, mask_01=soft, mode=\"normal\", dilate=2, thresh=0.05)\n",
    "            blended_i = poisson_blend_sample(\n",
    "            pred_rgb=pred_full_i,\n",
    "            base_rgb=rgb_i,\n",
    "            mask_01=soft_i,         # internally thresholded & slightly dilated\n",
    "            mode=\"normal\",\n",
    "            dilate=2,\n",
    "            thresh=0.3              # a bit higher than 0.05 → crisper mask\n",
    "            )\n",
    "            stem = f\"{idx:05d}\"\n",
    "            save_image(pred_masked_i.clamp(0,1), os.path.join(save_dir, f\"{stem}_pred_masked.png\"))\n",
    "            save_image(pred_full_i.clamp(0,1),   os.path.join(save_dir, f\"{stem}_pred_full.png\"))\n",
    "            save_image(blended_i.clamp(0,1),     os.path.join(save_dir, f\"{stem}_pred_blended.png\"))\n",
    "            save_triplet_fullsize(rgb_i, pred_masked_i, tgt_i, save_dir, stem)  # input/masked/targe                             \n",
    "            \n",
    "            # 1) always save full-size triplet (no stretch)\n",
    "            #save_triplet_fullsize(rgb_i, pred_i, tgt_i, save_dir, stem)\n",
    "\n",
    "            # 2) optional preview panel (letterboxed, not stretched)\n",
    "            if panel_mode == \"letterbox\":\n",
    "                tiles = [pad_to_square(x.clamp(0,1)) for x in (rgb_i, pred_i, tgt_i)]\n",
    "                # (optional) downscale large squares for disk space\n",
    "                if panel_size is not None:\n",
    "                    tiles = [F.interpolate(t.unsqueeze(0), size=(panel_size, panel_size),\n",
    "                                            mode=\"bilinear\", align_corners=False).squeeze(0)\n",
    "                             for t in tiles]\n",
    "                row = torch.stack(tiles, dim=0)  # [3,3,S,S]\n",
    "                save_image(row, os.path.join(save_dir, f\"{stem}_panel.png\"), nrow=3)\n",
    "\n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[validate] counts: img=67 mask=63 target=67\n",
      "[validate] common triples: 63\n",
      "[validate] missing masks for 4 imgs, e.g. ['WhatsApp Image 2025-07-12 at 5.46.30 PM', 'WhatsApp Image 2025-07-12 at 5.46.31 PM (2)', 'WhatsApp Image 2025-07-12 at 5.53.17 PM (1)', 'WhatsApp Image 2025-07-12 at 6.20.03 PM']\n"
     ]
    }
   ],
   "source": [
    "test_ds = NoseFolderDataset(\n",
    "    \"/workspace/data_splits/val/input\",\n",
    "    \"/workspace/data_splits/val/mask_input\",\n",
    "    \"/workspace/data_splits/val/target\"\n",
    ")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "test_loader = DataLoader(\n",
    "    test_ds, batch_size=2, shuffle=False, num_workers=2, pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "    collate_fn=lambda b: collate_keep_aspect(b, multiple=32)\n",
    ")\n",
    "\n",
    "run_test_unet(\n",
    "    ckpt_path=\"ckpts_Unet_PatchGan_Resv1/best_l1_mask.pt\",\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    save_dir=\"/workspace/results_unet_v3.1_postprocess\",\n",
    "    \n",
    "    panel_size=256,\n",
    "    amp=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8073816,
     "sourceId": 12803539,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
