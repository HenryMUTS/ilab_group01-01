{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/python\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (25.2)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "Collecting face_alignment\n",
      "  Using cached face_alignment-1.4.1-py2.py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting lpips\n",
      "  Using cached lpips-0.1.4-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting torchmetrics\n",
      "  Using cached torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting piq\n",
      "  Using cached piq-0.8.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from face_alignment) (2.8.0+cu128)\n",
      "Collecting scipy>=0.17 (from face_alignment)\n",
      "  Using cached scipy-1.16.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
      "Collecting scikit-image (from face_alignment)\n",
      "  Using cached scikit_image-0.25.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting opencv-python (from face_alignment)\n",
      "  Using cached opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Collecting tqdm (from face_alignment)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting numba (from face_alignment)\n",
      "  Using cached numba-0.62.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from lpips) (0.23.0+cu128)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Using cached fonttools-4.60.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (112 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /usr/lib/python3/dist-packages (from matplotlib) (3.1.1)\n",
      "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
      "  Using cached lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (80.9.0)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (3.19.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->face_alignment) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->face_alignment) (3.0.3)\n",
      "Collecting llvmlite<0.46,>=0.45.0dev0 (from numba->face_alignment)\n",
      "  Using cached llvmlite-0.45.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting imageio!=2.35.0,>=2.33 (from scikit-image->face_alignment)\n",
      "  Using cached imageio-2.37.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image->face_alignment)\n",
      "  Using cached tifffile-2025.10.4-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image->face_alignment)\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Using cached pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
      "Using cached face_alignment-1.4.1-py2.py3-none-any.whl (30 kB)\n",
      "Using cached lpips-0.1.4-py3-none-any.whl (53 kB)\n",
      "Using cached matplotlib-3.10.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "Using cached torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
      "Using cached piq-0.8.0-py3-none-any.whl (106 kB)\n",
      "Using cached contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (362 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.60.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (4.9 MB)\n",
      "Using cached kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.5 MB)\n",
      "Using cached lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached scipy-1.16.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.7 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached numba-0.62.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n",
      "Using cached llvmlite-0.45.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "Using cached opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (67.0 MB)\n",
      "Using cached scikit_image-0.25.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.0 MB)\n",
      "Using cached imageio-2.37.0-py3-none-any.whl (315 kB)\n",
      "Using cached lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Using cached tifffile-2025.10.4-py3-none-any.whl (230 kB)\n",
      "Installing collected packages: pytz, tzdata, tqdm, tifffile, scipy, opencv-python, llvmlite, lightning-utilities, lazy-loader, kiwisolver, imageio, fonttools, cycler, contourpy, scikit-image, pandas, numba, matplotlib, torchmetrics, face_alignment, piq, lpips\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/22\u001b[0m [lpips]m20/22\u001b[0m [piq]hmetrics]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed contourpy-1.3.3 cycler-0.12.1 face_alignment-1.4.1 fonttools-4.60.1 imageio-2.37.0 kiwisolver-1.4.9 lazy-loader-0.4 lightning-utilities-0.15.2 llvmlite-0.45.1 lpips-0.1.4 matplotlib-3.10.7 numba-0.62.1 opencv-python-4.12.0.88 pandas-2.3.3 piq-0.8.0 pytz-2025.2 scikit-image-0.25.2 scipy-1.16.2 tifffile-2025.10.4 torchmetrics-1.8.2 tqdm-4.67.1 tzdata-2025.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)  # should print /usr/bin/python\n",
    "!{sys.executable} -m pip install -U pip\n",
    "\n",
    "# install all your needed packages\n",
    "!{sys.executable} -m pip install pandas face_alignment lpips matplotlib torchmetrics piq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import face_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load pretrained landmark model (68 points, 2D)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth\" to /root/.cache/torch/hub/checkpoints/s3fd-619a316812.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85.7M/85.7M [00:07<00:00, 12.2MB/s]\n",
      "Downloading: \"https://www.adrianbulat.com/downloads/python-fan/2DFAN4-cd938726ad.zip\" to /root/.cache/torch/hub/checkpoints/2DFAN4-cd938726ad.zip\n",
      "100%|██████████| 91.9M/91.9M [00:09<00:00, 9.66MB/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fa = face_alignment.FaceAlignment(face_alignment.LandmarksType.TWO_D, device=device, flip_input=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Mask Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import face_alignment\n",
    "\n",
    "class FANNoseMaskGenerator:\n",
    "    def __init__(self, device=None):\n",
    "        if device is None:\n",
    "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.fa = face_alignment.FaceAlignment(face_alignment.LandmarksType.TWO_D, flip_input=False, device=device)\n",
    "        self.nose_idx = list(range(27, 36))  # 27–35 inclusive\n",
    "\n",
    "    def detect_nose_landmarks(self, img_rgb):\n",
    "        lms = self.fa.get_landmarks(img_rgb)\n",
    "        if lms is None or len(lms) == 0:\n",
    "            return [], False\n",
    "        nose_pts = lms[0][self.nose_idx, :2].astype(int).tolist()\n",
    "        return nose_pts, True\n",
    "\n",
    "    def create_mask(self, img_rgb, landmarks, dilate_px=35, up_shift=8, right_shift=6):\n",
    "        \"\"\"\n",
    "        1. Build convex-hull mask from nose landmarks.\n",
    "        2. Dilate isotropically (dilate_px).\n",
    "        3. Expand slightly upward and right (directional bias).\n",
    "    \n",
    "        Args:\n",
    "            dilate_px  : int  - large dilation radius (~30–40 fills full nostrils)\n",
    "            up_shift   : int  - pixels to extend mask upward (bridge)\n",
    "            right_shift: int  - pixels to extend mask rightward (toward nose tip)\n",
    "        \"\"\"\n",
    "        H, W = img_rgb.shape[:2]\n",
    "        mask = np.zeros((H, W), dtype=np.float32)\n",
    "        if not landmarks:\n",
    "            return mask\n",
    "    \n",
    "        # ---- base hull mask ----\n",
    "        pts = np.array(landmarks, dtype=np.int32)\n",
    "        hull = cv2.convexHull(pts)\n",
    "        cv2.fillConvexPoly(mask, hull, 1.0)\n",
    "    \n",
    "        # ---- large isotropic dilation ----\n",
    "        if dilate_px > 0:\n",
    "            k = dilate_px if dilate_px % 2 == 1 else dilate_px + 1\n",
    "            ker = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (k, k))\n",
    "            mask = cv2.dilate(mask, ker, iterations=1)\n",
    "    \n",
    "        # ---- directional expansion (up + right) ----\n",
    "        m = mask.copy()\n",
    "        if up_shift > 0:\n",
    "            roll_up = np.roll(mask, -up_shift, axis=0)\n",
    "            roll_up[-up_shift:, :] = 0  # zero bottom wrap\n",
    "            m = np.maximum(m, roll_up)\n",
    "    \n",
    "        if right_shift > 0:\n",
    "            roll_right = np.roll(mask, right_shift, axis=1)\n",
    "            roll_right[:, :right_shift] = 0  # zero left wrap\n",
    "            m = np.maximum(m, roll_right)\n",
    "    \n",
    "        return np.clip(m, 0, 1).astype(np.float32)\n",
    "\n",
    "    def visualize(self, img_rgb, mask, landmarks=None):\n",
    "        overlay = img_rgb.copy()\n",
    "        if landmarks:\n",
    "            for p in landmarks:\n",
    "                cv2.circle(overlay, p, 2, (0,255,0), -1)\n",
    "        blend = (0.7 * overlay + 0.3 * (mask[...,None]*np.array([255,0,0]))).astype(np.uint8)\n",
    "        return blend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/face_alignment/api.py:147: UserWarning: No faces were detected.\n",
      "  warnings.warn(\"No faces were detected.\")\n",
      "WARNING:root:Skipped (no landmarks): WhatsApp Image 2025-07-12 at 1.09.33 AM (1).jpeg -> No landmarks were detected.\n",
      "WARNING:root:Skipped (no landmarks): WhatsApp Image 2025-07-12 at 12.39.57 AM (3).jpeg -> No landmarks were detected.\n",
      "WARNING:root:Skipped (no landmarks): WhatsApp Image 2025-07-12 at 12.39.56 AM (2).jpeg -> No landmarks were detected.\n",
      "WARNING:root:Skipped (no landmarks): WhatsApp Image 2025-07-12 at 5.46.59 PM.jpeg -> No landmarks were detected.\n",
      "WARNING:root:Skipped (no landmarks): WhatsApp Image 2025-07-12 at 2.29.08 AM.jpeg -> No landmarks were detected.\n",
      "WARNING:root:Skipped (no landmarks): WhatsApp Image 2025-07-12 at 5.44.43 PM.jpeg -> No landmarks were detected.\n",
      "WARNING:root:Skipped (no landmarks): WhatsApp Image 2025-07-12 at 2.28.46 AM.jpeg -> No landmarks were detected.\n",
      "WARNING:root:Skipped (no landmarks): WhatsApp Image 2025-07-12 at 1.09.35 AM.jpeg -> No landmarks were detected.\n",
      "WARNING:root:Skipped (no landmarks): WhatsApp Image 2025-07-12 at 2.29.06 AM (2).jpeg -> No landmarks were detected.\n",
      "WARNING:root:Skipped (no landmarks): WhatsApp Image 2025-07-12 at 5.35.03 PM.jpeg -> No landmarks were detected.\n",
      "WARNING:root:Skipped (no landmarks): WhatsApp Image 2025-07-12 at 1.38.11 AM (2).jpeg -> No landmarks were detected.\n",
      "WARNING:root:Skipped (no landmarks): WhatsApp Image 2025-07-12 at 12.45.38 AM.jpeg -> No landmarks were detected.\n",
      "WARNING:root:Skipped (no landmarks): WhatsApp Image 2025-07-12 at 1.38.10 AM.jpeg -> No landmarks were detected.\n",
      "WARNING:root:Skipped (no landmarks): WhatsApp Image 2025-07-12 at 5.37.39 PM.jpeg -> No landmarks were detected.\n",
      "WARNING:root:Skipped (no landmarks): WhatsApp Image 2025-07-12 at 1.30.13 AM (6).jpeg -> No landmarks were detected.\n",
      "WARNING:root:Skipped (no landmarks): WhatsApp Image 2025-07-12 at 12.39.52 AM (1).jpeg -> No landmarks were detected.\n",
      "WARNING:root:Skipped (no landmarks): WhatsApp Image 2025-07-12 at 2.29.05 AM (1).jpeg -> No landmarks were detected.\n",
      "WARNING:root:Skipped (no landmarks): WhatsApp Image 2025-07-12 at 12.18.42 AM (2).jpeg -> No landmarks were detected.\n",
      "WARNING:root:Skipped (no landmarks): WhatsApp Image 2025-07-12 at 5.34.42 PM (2).jpeg -> No landmarks were detected.\n",
      "WARNING:root:Skipped (no landmarks): WhatsApp Image 2025-07-12 at 1.39.25 AM (1).jpeg -> No landmarks were detected.\n",
      "WARNING:root:Skipped (no landmarks): WhatsApp Image 2025-07-12 at 5.45.51 PM (1).jpeg -> No landmarks were detected.\n",
      "WARNING:root:Skipped (no landmarks): WhatsApp Image 2025-07-12 at 1.39.20 AM (1).jpeg -> No landmarks were detected.\n",
      "WARNING:root:Skipped (no landmarks): WhatsApp Image 2025-07-12 at 1.33.47 AM (4).jpeg -> No landmarks were detected.\n",
      "/usr/local/lib/python3.12/dist-packages/face_alignment/api.py:147: UserWarning: No faces were detected.\n",
      "  warnings.warn(\"No faces were detected.\")\n",
      "WARNING:root:Skipped (no landmarks): WhatsApp Image 2025-07-12 at 5.53.17 PM (1).jpeg -> No landmarks were detected.\n",
      "WARNING:root:Skipped (no landmarks): WhatsApp Image 2025-07-12 at 6.20.03 PM.jpeg -> No landmarks were detected.\n",
      "WARNING:root:Skipped (no landmarks): WhatsApp Image 2025-07-12 at 5.46.31 PM (2).jpeg -> No landmarks were detected.\n",
      "WARNING:root:Skipped (no landmarks): WhatsApp Image 2025-07-12 at 5.46.30 PM.jpeg -> No landmarks were detected.\n",
      "/usr/local/lib/python3.12/dist-packages/face_alignment/api.py:147: UserWarning: No faces were detected.\n",
      "  warnings.warn(\"No faces were detected.\")\n",
      "WARNING:root:Skipped (no landmarks): WhatsApp Image 2025-07-12 at 5.53.22 PM (1).jpeg -> No landmarks were detected.\n",
      "WARNING:root:Skipped (no landmarks): WhatsApp Image 2025-07-12 at 1.36.12 AM (2).jpeg -> No landmarks were detected.\n",
      "WARNING:root:Skipped (no landmarks): WhatsApp Image 2025-07-12 at 5.38.14 PM (1).jpeg -> No landmarks were detected.\n"
     ]
    }
   ],
   "source": [
    "import os, cv2, numpy as np, warnings, logging\n",
    "\n",
    "# ---- assume you already defined one of these elsewhere ----\n",
    "# detector = FANNoseMaskGenerator()         # FAN backend\n",
    "# detector = MediaPipeNoseDetector()        # MediaPipe backend\n",
    "\n",
    "def _ensure_dir(p): os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def _make_preview(img_rgb, mask01, alpha=0.35):\n",
    "    \"\"\"Return RGB preview with red mask overlay.\"\"\"\n",
    "    overlay = (mask01 * 255).astype(np.uint8)\n",
    "    vis = img_rgb.copy()\n",
    "    vis = cv2.addWeighted(vis, 1.0, cv2.cvtColor(overlay, cv2.COLOR_GRAY2RGB), alpha, 0)\n",
    "    return vis\n",
    "\n",
    "def create_nose_mask_from_image(\n",
    "    in_path, out_mask_path, out_prev_path, detector,\n",
    "    dilate_px=35, up_shift=8, right_shift=6, preview_alpha=0.35\n",
    "):\n",
    "    # --- load image (RGB) ---\n",
    "    img_bgr = cv2.imread(in_path, cv2.IMREAD_COLOR)\n",
    "    if img_bgr is None:\n",
    "        raise RuntimeError(f\"cv2.imread failed: {in_path}\")\n",
    "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # --- landmarks ---\n",
    "    landmarks, ok = detector.detect_nose_landmarks(img_rgb)\n",
    "    if not ok or len(landmarks) == 0:\n",
    "        raise Warning(\"No landmarks were detected.\")\n",
    "\n",
    "    # --- base hull ---\n",
    "    H, W = img_rgb.shape[:2]\n",
    "    mask = np.zeros((H, W), dtype=np.float32)\n",
    "    hull = cv2.convexHull(np.array(landmarks, dtype=np.int32))\n",
    "    cv2.fillConvexPoly(mask, hull, 1.0)\n",
    "\n",
    "    # --- isotropic dilation (fills full nose/alar) ---\n",
    "    if dilate_px > 0:\n",
    "        k = dilate_px if dilate_px % 2 == 1 else dilate_px + 1\n",
    "        ker = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (k, k))\n",
    "        mask = cv2.dilate(mask, ker, iterations=1)\n",
    "\n",
    "    # --- directional expansion: up + right (bridge/tip) ---\n",
    "    m = mask.copy()\n",
    "    if up_shift > 0:\n",
    "        r = np.roll(mask, -up_shift, axis=0); r[-up_shift:, :] = 0\n",
    "        m = np.maximum(m, r)\n",
    "    if right_shift > 0:\n",
    "        r = np.roll(mask,  right_shift, axis=1); r[:, :right_shift] = 0\n",
    "        m = np.maximum(m, r)\n",
    "\n",
    "    mask01 = np.clip(m, 0, 1).astype(np.float32)\n",
    "\n",
    "    # --- save mask (8-bit PNG) ---\n",
    "    _ensure_dir(os.path.dirname(out_mask_path))\n",
    "    cv2.imwrite(out_mask_path, (mask01 * 255).astype(np.uint8))\n",
    "\n",
    "    # --- save preview (JPG) ---\n",
    "    _ensure_dir(os.path.dirname(out_prev_path))\n",
    "    preview = _make_preview(img_rgb, mask01, alpha=preview_alpha)\n",
    "    cv2.imwrite(out_prev_path, cv2.cvtColor(preview, cv2.COLOR_RGB2BGR), [int(cv2.IMWRITE_JPEG_QUALITY), 95])\n",
    "\n",
    "def batch_create_nose_masks(src_dir, dst_dir, detector, exts=('.jpg','.jpeg','.png'),\n",
    "                            dilate_px=35, up_shift=8, right_shift=6, preview_alpha=0.35):\n",
    "    \"\"\"\n",
    "    Generate masks + previews for all images in src_dir.\n",
    "    Masks -> dst_dir; Previews -> dst_dir/_preview\n",
    "    \"\"\"\n",
    "    _ensure_dir(dst_dir)\n",
    "    prev_dir = os.path.join(dst_dir, \"_preview\"); _ensure_dir(prev_dir)\n",
    "    warnings.filterwarnings(\"error\", message=\"No landmarks were detected.\")\n",
    "\n",
    "    for fname in os.listdir(src_dir):\n",
    "        if not fname.lower().endswith(exts): \n",
    "            continue\n",
    "        stem, _ = os.path.splitext(fname)\n",
    "        in_path   = os.path.join(src_dir, fname)\n",
    "        mask_path = os.path.join(dst_dir, f\"{stem}.png\")\n",
    "        prev_path = os.path.join(prev_dir, f\"{stem}.jpg\")\n",
    "\n",
    "        if os.path.exists(mask_path) and os.path.exists(prev_path):\n",
    "            logging.info(f\"Skip existing: {fname}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            create_nose_mask_from_image(\n",
    "                in_path, mask_path, prev_path, detector,\n",
    "                dilate_px=dilate_px, up_shift=up_shift, right_shift=right_shift,\n",
    "                preview_alpha=preview_alpha\n",
    "            )\n",
    "        except Warning as w:\n",
    "            logging.warning(f\"Skipped (no landmarks): {fname} -> {w}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error on {fname}: {e}\")\n",
    "\n",
    "# ---- Run for all splits ----\n",
    "base_dir = \"/workspace/data_splits\"\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "detector = FANNoseMaskGenerator()  # or MediaPipeNoseDetector()\n",
    "\n",
    "for split in splits:\n",
    "    input_folder = os.path.join(base_dir, split, \"input\")\n",
    "    mask_folder  = os.path.join(base_dir, split, \"mask_input_new\")\n",
    "    batch_create_nose_masks(\n",
    "        input_folder, mask_folder, detector,\n",
    "        dilate_px=35, up_shift=8, right_shift=6, preview_alpha=0.35\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old Nose Mask Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nose_mask(image_path, save_path, log):\n",
    "    try:\n",
    "        img = cv2.imread(image_path)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "        preds = fa.get_landmarks(img_rgb)\n",
    "        if preds is None:\n",
    "            return False\n",
    "            \n",
    "    except Warning as w:  # Catch warnings as exceptions\n",
    "        return False\n",
    "\n",
    "    landmarks = preds[0]\n",
    "    nose_points = landmarks[27:36]  # indexes 27–35 in 0-based Python index\n",
    "\n",
    "    mask = np.zeros(img.shape[:2], dtype=np.uint8)\n",
    "    cv2.fillConvexPoly(mask, np.int32(nose_points), 255)\n",
    "\n",
    "    cv2.imwrite(save_path, mask)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "def batch_create_nose_masks(src_dir, dst_dir, create_func, exts=('.jpg','.jpeg','.png')):\n",
    "    \"\"\"\n",
    "    Generate nose masks for all images in src_dir and save in dst_dir.\n",
    "    Skips files if the mask already exists.\n",
    "    \"\"\"\n",
    "    os.makedirs(dst_dir, exist_ok=True)\n",
    "    warnings.filterwarnings(\"error\", message=\"No faces were detected.\")\n",
    "\n",
    "    for file in os.listdir(src_dir):\n",
    "        if file.lower().endswith(exts):\n",
    "            in_path  = os.path.join(src_dir, file)\n",
    "            out_path = os.path.join(dst_dir, file)\n",
    "            if not os.path.exists(out_path):\n",
    "                try:\n",
    "                    create_func(in_path, out_path, logging)\n",
    "                except Warning as w:\n",
    "                    logging.warning(f\"Skipped {file}: {w}\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error with {file}: {e}\")\n",
    "\n",
    "base_dir = \"/workspace/data_splits\"\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "for split in splits:\n",
    "    input_folder = os.path.join(base_dir, split, \"input\")\n",
    "    mask_folder  = os.path.join(base_dir, split, \"mask_input\")\n",
    "    batch_create_nose_masks(input_folder, mask_folder, create_nose_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\"}\n",
    "\n",
    "def _map_stem_to_name(dir_path: Path):\n",
    "    \"\"\"Return {stem: filename} for allowed image files (first match per stem).\"\"\"\n",
    "    m = {}\n",
    "    for p in dir_path.iterdir():\n",
    "        if p.is_file() and p.suffix.lower() in IMG_EXTS:\n",
    "            m.setdefault(p.stem, p.name)  # keep first occurrence\n",
    "    return m\n",
    "\n",
    "def validate_files(img_dir, mask_dir, target_dir, verbose=True):\n",
    "    img_dir, mask_dir, target_dir = map(Path, (img_dir, mask_dir, target_dir))\n",
    "\n",
    "    img_map    = _map_stem_to_name(img_dir)\n",
    "    mask_map   = _map_stem_to_name(mask_dir)\n",
    "    target_map = _map_stem_to_name(target_dir)\n",
    "\n",
    "    img_stems    = set(img_map.keys())\n",
    "    mask_stems   = set(mask_map.keys())\n",
    "    target_stems = set(target_map.keys())\n",
    "\n",
    "    common = sorted(img_stems & mask_stems & target_stems)\n",
    "\n",
    "    missing_mask   = sorted(img_stems - mask_stems)\n",
    "    missing_target = sorted(img_stems - target_stems)\n",
    "    orphan_masks   = sorted(mask_stems - img_stems)\n",
    "    orphan_targets = sorted(target_stems - img_stems)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[validate] counts: img={len(img_stems)} mask={len(mask_stems)} target={len(target_stems)}\")\n",
    "        print(f\"[validate] common triples: {len(common)}\")\n",
    "        if missing_mask:   print(f\"[validate] missing masks for {len(missing_mask)} imgs, e.g. {missing_mask[:5]}\")\n",
    "        if missing_target: print(f\"[validate] missing targets for {len(missing_target)} imgs, e.g. {missing_target[:5]}\")\n",
    "        if orphan_masks:   print(f\"[validate] masks without imgs: {len(orphan_masks)}, e.g. {orphan_masks[:5]}\")\n",
    "        if orphan_targets: print(f\"[validate] targets without imgs: {len(orphan_targets)}, e.g. {orphan_targets[:5]}\")\n",
    "\n",
    "    # Build (img_filename, target_filename) using the exact filenames found\n",
    "    pairs = [(img_map[s], target_map[s]) for s in common]\n",
    "\n",
    "    if len(pairs) == 0:\n",
    "        raise ValueError(\n",
    "            \"No valid (img, mask, target) triples found.\\n\"\n",
    "            f\"Checked:\\n  img_dir={img_dir}\\n  mask_dir={mask_dir}\\n  target_dir={target_dir}\\n\"\n",
    "            \"See [validate] logs above for mismatches.\"\n",
    "        )\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def collate_keep_aspect(batch, multiple=32):\n",
    "    \"\"\"\n",
    "    Batch collate that keeps aspect ratios and preserves metadata (fname, file paths).\n",
    "    Pads all images/masks/targets in the batch to the max H and W,\n",
    "    rounded up to the nearest multiple.\n",
    "    \"\"\"\n",
    "    imgs, tgts, masks, orig_hw = [], [], [], []\n",
    "    fnames, input_files, mask_files, target_files = [], [], [], []\n",
    "\n",
    "    for b in batch:\n",
    "        x = b[\"input\"]    # [3,H,W]\n",
    "        y = b[\"target\"]   # [3,H,W]\n",
    "        m = b[\"mask\"]     # [1,H,W]\n",
    "        H, W = x.shape[1:]\n",
    "\n",
    "        imgs.append(x)\n",
    "        tgts.append(y)\n",
    "        masks.append(m)\n",
    "        orig_hw.append(torch.tensor([H, W], dtype=torch.int32))\n",
    "\n",
    "        # preserve meta fields\n",
    "        fnames.append(b.get(\"fname\", \"\"))\n",
    "        input_files.append(b.get(\"input_file\", \"\"))\n",
    "        mask_files.append(b.get(\"mask_file\", \"\"))\n",
    "        target_files.append(b.get(\"target_file\", \"\"))\n",
    "\n",
    "    # find max height/width in this batch\n",
    "    Ht = max(t.shape[1] for t in imgs)\n",
    "    Wt = max(t.shape[2] for t in imgs)\n",
    "\n",
    "    # round up to nearest multiple (stride)\n",
    "    Ht = (Ht + multiple - 1) // multiple * multiple\n",
    "    Wt = (Wt + multiple - 1) // multiple * multiple\n",
    "\n",
    "    def pad_to(t, Ht, Wt):\n",
    "        # pad as (left, right, top, bottom)\n",
    "        return F.pad(t, (0, Wt - t.shape[2], 0, Ht - t.shape[1]))\n",
    "\n",
    "    X = torch.stack([pad_to(t, Ht, Wt) for t in imgs])\n",
    "    Y = torch.stack([pad_to(t, Ht, Wt) for t in tgts])\n",
    "    M = torch.stack([pad_to(t, Ht, Wt) for t in masks])\n",
    "    OHW = torch.stack(orig_hw)\n",
    "\n",
    "    return {\n",
    "        \"input\": X,     # [B,3,Ht,Wt]\n",
    "        \"target\": Y,    # [B,3,Ht,Wt]\n",
    "        \"mask\": M,      # [B,1,Ht,Wt]\n",
    "        \"orig_hw\": OHW, # [B,2]\n",
    "        \"fname\": fnames,\n",
    "        \"input_file\": input_files,\n",
    "        \"mask_file\": mask_files,\n",
    "        \"target_file\": target_files\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "import torch, cv2\n",
    "import numpy as np\n",
    "\n",
    "class NoseFolderDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, target_dir, size=256):\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.mask_dir = Path(mask_dir)\n",
    "        self.target_dir = Path(target_dir)\n",
    "        self.file_pairs = validate_files(self.img_dir, self.mask_dir, self.target_dir)   # [(img_name, tgt_name), ...]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_pairs)\n",
    "\n",
    "    def _read_rgb(self, path):\n",
    "        img = cv2.imread(str(path), cv2.IMREAD_COLOR)\n",
    "        if img is None:\n",
    "            raise FileNotFoundError(path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)        # (H,W,3) uint8\n",
    "        return img\n",
    "\n",
    "    import os, re, cv2, glob, numpy as np\n",
    "\n",
    "    def _read_mask(self, path):\n",
    "        \"\"\"\n",
    "        Reads a mask robustly even if:\n",
    "          - extension differs (.png, .jpg, .jpeg)\n",
    "          - name was sanitized (spaces -> _ etc.)\n",
    "        Falls back to empty mask if not found.\n",
    "        \"\"\"\n",
    "        # ---- try exact path first ----\n",
    "        if os.path.exists(path):\n",
    "            m = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)\n",
    "            if m is not None:\n",
    "                return (m.astype(np.float32) / 255.0)\n",
    "    \n",
    "        # ---- build stem & try alternate extensions ----\n",
    "        stem = os.path.splitext(os.path.basename(path))[0]\n",
    "        mask_dir = os.path.dirname(path)\n",
    "        for ext in (\".png\", \".jpg\", \".jpeg\"):\n",
    "            alt = os.path.join(mask_dir, stem + ext)\n",
    "            if os.path.exists(alt):\n",
    "                m = cv2.imread(str(alt), cv2.IMREAD_GRAYSCALE)\n",
    "                if m is not None:\n",
    "                    return (m.astype(np.float32) / 255.0)\n",
    "    \n",
    "        # ---- try sanitized name ----\n",
    "        s2 = re.sub(r'[^A-Za-z0-9._-]+', '_', stem)\n",
    "        s2 = re.sub(r'_+', '_', s2).strip('_')\n",
    "        for ext in (\".png\", \".jpg\", \".jpeg\"):\n",
    "            alt = os.path.join(mask_dir, s2 + ext)\n",
    "            if os.path.exists(alt):\n",
    "                m = cv2.imread(str(alt), cv2.IMREAD_GRAYSCALE)\n",
    "                if m is not None:\n",
    "                    return (m.astype(np.float32) / 255.0)\n",
    "    \n",
    "        # ---- final fallback: try any matching prefix ----\n",
    "        hits = glob.glob(os.path.join(mask_dir, stem + \".*\"))\n",
    "        hits = [h for h in hits if os.path.splitext(h)[1].lower() in (\".png\", \".jpg\", \".jpeg\")]\n",
    "        if hits:\n",
    "            m = cv2.imread(hits[0], cv2.IMREAD_GRAYSCALE)\n",
    "            if m is not None:\n",
    "                return (m.astype(np.float32) / 255.0)\n",
    "    \n",
    "        # ---- completely missing: return blank mask (same H,W as input image later) ----\n",
    "        print(f\"[WARN] Missing mask for: {path}\")\n",
    "        return np.zeros((512, 512), np.float32)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname, target_f = self.file_pairs[idx]\n",
    "        ip, mp, tp = self.img_dir/fname, self.mask_dir/fname, self.target_dir/target_f\n",
    "\n",
    "        img    = self._read_rgb(ip)     # H,W,3\n",
    "        target = self._read_rgb(tp)     # H,W,3\n",
    "        mask   = self._read_mask(mp)    # H,W\n",
    "        \n",
    "        if img.shape[:2] != target.shape[:2]:\n",
    "            # minimal: resize target to img (or vice-versa, pick one consistently)\n",
    "            target = cv2.resize(target, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_AREA)\n",
    "        if img.shape[:2] != mask.shape[:2]:\n",
    "            mask = cv2.resize(mask, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        H, W = img.shape[:2]\n",
    "        img_t  = torch.from_numpy(img).permute(2,0,1).float()/255.0\n",
    "        tgt_t  = torch.from_numpy(target).permute(2,0,1).float()/255.0\n",
    "        mask_t = torch.from_numpy(mask).unsqueeze(0).float()\n",
    "\n",
    "        return {\n",
    "            \"input\": img_t, \"target\": tgt_t, \"mask\": mask_t,\n",
    "            \"orig_h\": H, \"orig_w\": W,\n",
    "            \"input_file\": str(ip), \"mask_file\": str(mp), \"target_file\": str(tp),\n",
    "            \"fname\": os.path.basename(fname)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- Generator (UnE) ----------\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------- UNet blocks ---------------------------\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(out_ch, affine=True),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(out_ch, affine=True),\n",
    "            nn.SiLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x): return self.block(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.conv = ConvBlock(in_ch, out_ch)\n",
    "    def forward(self, x): return self.conv(self.pool(x))\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.up   = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        self.conv = ConvBlock(in_ch, out_ch)\n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        # pad if needed (in case odd dims)\n",
    "        diffY = skip.size(2) - x.size(2)\n",
    "        diffX = skip.size(3) - x.size(3)\n",
    "        if diffY != 0 or diffX != 0:\n",
    "            x = F.pad(x, (0, diffX, 0, diffY))\n",
    "        x = torch.cat([skip, x], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "# --------------------------- UNet Generator (residual) ---------------------------\n",
    "class UNetNoseGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    UNet that takes RGB+mask (4ch) and predicts a 3ch residual 'delta'.\n",
    "    Output = rgb + soft_mask * tanh(raw) * scale\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch=4, out_ch=3, base=64, depth=5, res_max=1):\n",
    "        \"\"\"\n",
    "        depth=5 -> downsample x2 five times (stride 32). Use collate multiple=32.\n",
    "        For less memory, set depth=4 (stride 16).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.out_ch = out_ch\n",
    "        self.res_max = res_max\n",
    "\n",
    "        # encoder\n",
    "        self.inc  = ConvBlock(in_ch, base)                 # H\n",
    "        self.down1 = Down(base,     base*2)                # H/2\n",
    "        self.down2 = Down(base*2,   base*4)                # H/4\n",
    "        self.down3 = Down(base*4,   base*8)                # H/8\n",
    "        self.down4 = Down(base*8,   base*8)                # H/16\n",
    "        self.has_down5 = (depth >= 5)\n",
    "        if self.has_down5:\n",
    "            self.down5 = Down(base*8, base*8)              # H/32\n",
    "\n",
    "        # decoder\n",
    "        if self.has_down5:\n",
    "            self.up1 = Up(base*8 + base*8, base*8)         # concat with down4\n",
    "            ch_up_in = base*8 + base*8\n",
    "        else:\n",
    "            # if no down5, first up will concatenate down3 and bottleneck at base*8\n",
    "            ch_up_in = base*8 + base*8  # consistent with next lines\n",
    "\n",
    "        self.up2 = Up(base*8 + base*8, base*8)             # + down3\n",
    "        self.up3 = Up(base*8 + base*4, base*4)             # + down2\n",
    "        self.up4 = Up(base*4 + base*2, base*2)             # + down1\n",
    "        self.up5 = Up(base*2 + base,   base)               # + inc\n",
    "\n",
    "        self.outc = nn.Conv2d(base, out_ch, kernel_size=3, padding=1)\n",
    "        nn.init.zeros_(self.outc.weight)\n",
    "        nn.init.zeros_(self.outc.bias)\n",
    "\n",
    "        # learnable residual cap\n",
    "        self._alpha = nn.Parameter(torch.tensor(0.0))  # sigmoid ~0.5 initially\n",
    "\n",
    "    def forward(self, inp, return_full=False):\n",
    "        \"\"\"\n",
    "        inp: [B,4,H,W]  (RGB + binary/soft mask in channel 4)\n",
    "        returns: [B,3,H,W] blended output at original size\n",
    "        \"\"\"\n",
    "        rgb   = inp[:, :3]\n",
    "        mask1 = inp[:, 3:4]\n",
    "\n",
    "        # encoder\n",
    "        x1 = self.inc(inp)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        if self.has_down5:\n",
    "            x6 = self.down5(x5)\n",
    "            u1 = self.up1(x6, x5)\n",
    "        else:\n",
    "            u1 = self.up2(x5, x4)  # skip one level if depth=4\n",
    "\n",
    "        # decoder path\n",
    "        if self.has_down5:\n",
    "            u2 = self.up2(u1, x4)\n",
    "        else:\n",
    "            u2 = u1\n",
    "        u3 = self.up3(u2, x3)\n",
    "        u4 = self.up4(u3, x2)\n",
    "        u5 = self.up5(u4, x1)\n",
    "\n",
    "        raw   = self.outc(u5)               # [B,3,H,W]\n",
    "        \n",
    "        delta = raw\n",
    "        H, W = rgb.shape[-2:]\n",
    "\n",
    "        if delta.shape[-2:] != (H, W):\n",
    "            \n",
    "            delta = F.interpolate(delta, size=(H, W), mode='bilinear', align_corners=False)\n",
    "        hard   = mask1\n",
    "        if hard.shape[-2:] != (H, W):\n",
    "            hard = F.interpolate(hard, size=(H, W), mode='nearest').clamp(0, 1)\n",
    "\n",
    "        m_soft = hard\n",
    "        m3     = m_soft.repeat(1, 3, 1, 1)\n",
    "        full_rgb = (rgb + delta)\n",
    "        out    = rgb + delta * m3\n",
    "        if return_full:\n",
    "            return out, full_rgb, m_soft\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "class VGGPerceptualLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    VGG16 feature L1 loss. Works on inputs in [0,1].\n",
    "    If mask is given (B,1,H,W), it is downsampled per VGG stage and used\n",
    "    to weight the feature differences spatially.\n",
    "    \"\"\"\n",
    "    def __init__(self, layers=(3, 8, 15, 22), layer_weights=None):\n",
    "        super().__init__()\n",
    "        # relu1_2=3, relu2_2=8, relu3_3=15, relu4_3=22 in torchvision VGG16.features\n",
    "        self.layers = tuple(layers)\n",
    "        self.layer_weights = (\n",
    "            [1.0] * len(self.layers) if layer_weights is None else layer_weights\n",
    "        )\n",
    "\n",
    "        vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_FEATURES).features\n",
    "        self.vgg = vgg.eval()\n",
    "        for p in self.vgg.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "        # register mean/std buffers for ImageNet normalization\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1)\n",
    "        std  = torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1)\n",
    "        self.register_buffer('mean', mean, persistent=False)\n",
    "        self.register_buffer('std',  std,  persistent=False)\n",
    "\n",
    "    def _norm(self, x):\n",
    "        # x expected in [0,1]\n",
    "        return (x - self.mean) / self.std\n",
    "\n",
    "    def forward(self, pred, target, mask=None):\n",
    "        \"\"\"\n",
    "        pred, target: (B,3,H,W) in [0,1]\n",
    "        mask (optional): (B,1,H,W) with 0..1 weights (e.g., your nose mask)\n",
    "        \"\"\"\n",
    "        x = self._norm(pred)\n",
    "        y = self._norm(target)\n",
    "\n",
    "        loss = 0.0\n",
    "        idx_set = set(self.layers)\n",
    "        lw_iter = iter(self.layer_weights)\n",
    "\n",
    "        # run through the VGG and collect selected layers\n",
    "        for i, layer in enumerate(self.vgg):\n",
    "            x = layer(x)\n",
    "            y = layer(y)\n",
    "\n",
    "            if i in idx_set:\n",
    "                w = next(lw_iter)\n",
    "                if mask is None:\n",
    "                    # plain feature L1\n",
    "                    loss += w * F.l1_loss(x, y)\n",
    "                else:\n",
    "                    # masked feature L1 (downsample mask to feature size)\n",
    "                    m = F.interpolate(mask, size=x.shape[-2:], mode='bilinear', align_corners=False)\n",
    "                    num = (m * (x - y).abs()).sum()\n",
    "                    den = m.sum() + 1e-6\n",
    "                    loss += w * (num / den)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.image.ssim import StructuralSimilarityIndexMeasure\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "def _pad_to_match(t, target_h, target_w):\n",
    "    \"\"\"Pad a tensor (C,H,W) with zeros to match target size.\"\"\"\n",
    "    _, h, w = t.shape\n",
    "    pad_h = target_h - h\n",
    "    pad_w = target_w - w\n",
    "    if pad_h == 0 and pad_w == 0:\n",
    "        return t\n",
    "    return torch.nn.functional.pad(t, (0, pad_w, 0, pad_h))\n",
    "\n",
    "    \n",
    "\n",
    "def _find_index_by_stem(dataset, stem: str):\n",
    "    \n",
    "    \"\"\"Find dataset index by filename stem (ignores extension).\"\"\"\n",
    "    stem = Path(stem).stem  # handle if user passed full name\n",
    "    for i in range(len(dataset)):\n",
    "        # dataset.file_pairs[i] = (img_name, tgt_name)\n",
    "        img_name = dataset.file_pairs[i][0]\n",
    "        if Path(img_name).stem == stem:\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def _find_index_by_stem(dataset, stem: str):\n",
    "    stem = Path(stem).stem\n",
    "    for i in range(len(dataset)):\n",
    "        img_name = dataset.file_pairs[i][0]\n",
    "        if Path(img_name).stem == stem:\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "def _resolve_indices(dataset, extra_ids):\n",
    "    \"\"\"\n",
    "    extra_ids: list of stems/filenames (str) or integer indices.\n",
    "    Returns: list of valid, unique dataset indices in given order.\n",
    "    \"\"\"\n",
    "    seen = set()\n",
    "    resolved = []\n",
    "    for x in (extra_ids or []):\n",
    "        idx = None\n",
    "        if isinstance(x, int):\n",
    "            if 0 <= x < len(dataset):\n",
    "                idx = x\n",
    "        else:\n",
    "            idx = _find_index_by_stem(dataset, str(x))\n",
    "        if idx is None:\n",
    "            print(f\"[WARN] Extra sample not found: {x}\")\n",
    "            continue\n",
    "        if idx in seen:\n",
    "            continue\n",
    "        seen.add(idx)\n",
    "        resolved.append(idx)\n",
    "    return resolved\n",
    "\n",
    "@torch.no_grad()\n",
    "def save_vis_grid(\n",
    "    G, loader, device, out_dir, epoch, n=6,\n",
    "    extra_names=None,         # <--- NEW: list[str|int] (stems or indices)\n",
    "    max_extra=None            # <--- NEW: optional cap on how many extras to include\n",
    "):\n",
    "    \"\"\"\n",
    "    Save a grid with n samples from first batch + optional extra samples.\n",
    "    Each row: [Input | Prediction | Target]\n",
    "    - extra_names: list of filename stems (or integer indices) to add as extra rows.\n",
    "    - max_extra: limit number of extras (None = no cap).\n",
    "    \"\"\"\n",
    "    G.eval()\n",
    "    os.makedirs(os.path.join(out_dir, \"samples\"), exist_ok=True)\n",
    "\n",
    "    # ---- first batch ----\n",
    "    try:\n",
    "        batch = next(iter(loader))\n",
    "    except StopIteration:\n",
    "        print(\"[WARN] Loader is empty.\")\n",
    "        return\n",
    "\n",
    "    rgb    = batch[\"input\"].to(device)\n",
    "    target = batch[\"target\"].to(device)\n",
    "    mask   = batch[\"mask\"].to(device)\n",
    "    pred   = G(torch.cat([rgb, mask], dim=1)).clamp(0, 1)\n",
    "\n",
    "    tiles = []\n",
    "    k = min(n, rgb.size(0))\n",
    "    for i in range(k):\n",
    "        tiles += [rgb[i], pred[i], target[i]]\n",
    "\n",
    "    # ---- extras (multiple) ----\n",
    "    extra_count = 0\n",
    "    if hasattr(loader, \"dataset\") and extra_names:\n",
    "        idxs = _resolve_indices(loader.dataset, extra_names)\n",
    "        if max_extra is not None:\n",
    "            idxs = idxs[:max_extra]\n",
    "\n",
    "        for idx in idxs:\n",
    "            try:\n",
    "                sample = loader.dataset[idx]\n",
    "                rgb_e  = sample[\"input\"].unsqueeze(0).to(device)\n",
    "                tgt_e  = sample[\"target\"].unsqueeze(0).to(device)\n",
    "                mask_e = sample[\"mask\"].unsqueeze(0).to(device)\n",
    "                pred_e = G(torch.cat([rgb_e, mask_e], dim=1)).clamp(0, 1)\n",
    "\n",
    "                # append as a new row\n",
    "                tiles += [rgb_e[0], pred_e[0], tgt_e[0]]\n",
    "                # best-effort name for logging\n",
    "                try:\n",
    "                    name = Path(loader.dataset.file_pairs[idx][0]).stem\n",
    "                except Exception:\n",
    "                    name = f\"idx_{idx}\"\n",
    "                print(f\"[INFO] Added extra row: {name}\")\n",
    "                extra_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Could not process extra idx={idx}: {e}\")\n",
    "    elif extra_names and not hasattr(loader, \"dataset\"):\n",
    "        print(\"[WARN] Loader has no dataset; cannot resolve extra_names.\")\n",
    "\n",
    "    # ---- pad before grid (handles size mismatches) ----\n",
    "    H = max(t.shape[1] for t in tiles)\n",
    "    W = max(t.shape[2] for t in tiles)\n",
    "    tiles = [_pad_to_match(t, H, W) for t in tiles]\n",
    "\n",
    "    grid = make_grid(tiles, nrow=3, padding=2)\n",
    "    out_path = os.path.join(out_dir, \"samples\", f\"ep_{epoch:03d}.png\")\n",
    "    save_image(grid, out_path)\n",
    "    print(f\"[OK] Saved visualization grid → {out_path} (extras added: {extra_count})\")\n",
    "\n",
    "    G.train()\n",
    "\n",
    "\"\"\"\n",
    "def dilate_mask_binary(mask, k=14, iters=2):\n",
    "   Binary dilation. mask: (B,1,H,W) in {0,1} float.\n",
    "\n",
    "    out = mask\n",
    "    for _ in range(iters):\n",
    "        out = F.max_pool2d(out, kernel_size=k, stride=1, padding=k//2)\n",
    "    return out.clamp(0,1)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def erode_mask_binary(mask, k=11, iters=1):\n",
    "    \"\"\"Binary erosion via max-pool trick.\"\"\"\n",
    "    x = 1.0 - mask\n",
    "    for _ in range(iters):\n",
    "        x = F.max_pool2d(x, kernel_size=k, stride=1, padding=k//2)\n",
    "    return (1.0 - x).clamp(0,1)\n",
    "\n",
    "\"\"\"\n",
    "def feather_mask(mask, k=5):\n",
    "   Soft edge (0..1) via avg-pool (feathered band).\n",
    "\n",
    "    return F.avg_pool2d(mask, kernel_size=k, stride=1, padding=k//2).clamp(0,1)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# ---------- simple PatchGAN (1-channel real/fake map) ----------\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, in_ch=6, base=64):\n",
    "        super().__init__()\n",
    "        c = base\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch,   c, 4, 2, 1), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(c,     c*2, 4, 2, 1), nn.BatchNorm2d(c*2), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(c*2,   c*4, 4, 2, 1), nn.BatchNorm2d(c*4), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(c*4,   c*8, 4, 2, 1), nn.BatchNorm2d(c*8), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(c*8,     1, 4, 1, 1)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "\n",
    "\n",
    "import os, torch, torch.nn as nn, torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import piq\n",
    "from torchmetrics.image.ssim import StructuralSimilarityIndexMeasure\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def crop_to_mask_bbox(t: torch.Tensor, m: torch.Tensor, thresh: float = 0.05, dilate_px: int = 0):\n",
    "    \"\"\"\n",
    "    t: [B,C,H,W], m: [B,1,H,W] in [0,1]; returns batch of crops stacked.\n",
    "    We find a tight bbox of (m>thresh). Optionally dilate bbox by 'dilate_px'.\n",
    "    If mask is empty, fall back to full image for that item.\n",
    "    \"\"\"\n",
    "    B, _, H, W = t.shape\n",
    "    crops = []\n",
    "    for b in range(B):\n",
    "        mb = m[b, 0]\n",
    "        yy, xx = (mb > thresh).nonzero(as_tuple=True)\n",
    "        if yy.numel() == 0:\n",
    "            crops.append(t[b:b+1])\n",
    "            continue\n",
    "        y0, y1 = yy.min().item(), yy.max().item() + 1\n",
    "        x0, x1 = xx.min().item(), xx.max().item() + 1\n",
    "        if dilate_px > 0:\n",
    "            y0 = max(0, y0 - dilate_px); x0 = max(0, x0 - dilate_px)\n",
    "            y1 = min(H, y1 + dilate_px); x1 = min(W, x1 + dilate_px)\n",
    "        crops.append(t[b:b+1, :, y0:y1, x0:x1])\n",
    "    return torch.cat(crops, dim=0)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_epoch(\n",
    "    G, loader, device, ssim_metric,\n",
    "    epoch: int = 0,\n",
    "    save_debug: bool = True,\n",
    "    show_debug: bool = True,\n",
    "    perceptual_fn=None,          # <- pass your make_perceptual_fn(...) result here\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        mean_l1_mask, mean_ssim_global, mean_ssim_mask, mean_lpips\n",
    "    Notes:\n",
    "        - LPIPS is computed via the provided perceptual_fn (masked + ROI).\n",
    "        - If perceptual_fn is None, LPIPS is reported as 0.0.\n",
    "    \"\"\"\n",
    "    G.eval()\n",
    "    tot_l1m = tot_ssim_global = tot_ssim_mask = tot_lpips = 0.0\n",
    "    n = 0\n",
    "    debug_done = False\n",
    "\n",
    "    for batch in loader:\n",
    "        rgb    = batch[\"input\"].to(device)   # [B,3,H,W] in [0,1]\n",
    "        target = batch[\"target\"].to(device)  # [B,3,H,W]\n",
    "        mask   = batch[\"mask\"].to(device)    # [B,1,H,W]\n",
    "        inp    = torch.cat([rgb, mask], dim=1)\n",
    "\n",
    "        # hard mask (no feather), resize if needed\n",
    "        H, W = rgb.shape[-2:]\n",
    "        hard = mask\n",
    "        if hard.shape[-2:] != (H, W):\n",
    "            hard = F.interpolate(hard, size=(H, W), mode='nearest').clamp(0, 1)\n",
    "        m_soft = hard\n",
    "\n",
    "        # forward (support return_full=True if available)\n",
    "        try:\n",
    "            pred, full_rgb, _ = G(inp, return_full=True)\n",
    "        except TypeError:\n",
    "            pred = G(inp)\n",
    "            full_rgb = pred\n",
    "\n",
    "        # ---- scalar metrics\n",
    "        l1m = ((pred - target).abs() * m_soft).sum() / (m_soft.sum() + 1e-6)\n",
    "        ssim_global = ssim_metric(pred, target)\n",
    "        ssim_masked = ssim_metric(pred * m_soft, target * m_soft)\n",
    "\n",
    "        # LPIPS via your perceptual_fn (masked + ROI); returns scalar tensor\n",
    "        if perceptual_fn is not None:\n",
    "            lp = perceptual_fn(pred, target, mask=m_soft)\n",
    "            lpips_val = float(lp.detach().item())\n",
    "        else:\n",
    "            lpips_val = 0.0\n",
    "\n",
    "        b = inp.size(0)\n",
    "        tot_l1m         += l1m.item() * b\n",
    "        tot_ssim_global += ssim_global.item() * b\n",
    "        tot_ssim_mask   += ssim_masked.item() * b\n",
    "        tot_lpips       += lpips_val * b\n",
    "        n += b\n",
    "\n",
    "        # ---- one-time DEBUG PANEL\n",
    "        if (not debug_done) and (show_debug or save_debug):\n",
    "            debug_done = True\n",
    "\n",
    "            d0 = (full_rgb - rgb)[0].permute(1,2,0).cpu().numpy()  # HxWx3\n",
    "            dvis = np.clip(d0 * 0.5 + 0.5, 0, 1)\n",
    "\n",
    "            m0  = (mask[0,0].detach().cpu().numpy().astype(np.uint8) * 255)\n",
    "            import cv2\n",
    "            edges = cv2.Canny(m0, 0, 1) > 0\n",
    "            overlay = (dvis * 255).astype(np.uint8)\n",
    "            overlay[edges] = np.array([255, 64, 64], dtype=np.uint8)  # red outline\n",
    "            overlay = overlay.astype(np.float32) / 255.0\n",
    "\n",
    "            sign_vis = np.sign(d0).mean(axis=2)\n",
    "            sign_vis = np.clip((sign_vis + 1) / 2.0, 0, 1)\n",
    "            sign_vis = plt.cm.seismic(sign_vis)[..., :3]\n",
    "\n",
    "            fig, axs = plt.subplots(1, 2, figsize=(8, 6))\n",
    "            axs[0].imshow(overlay);  axs[0].set_title(\"Delta vis + mask edge\"); axs[0].axis('off')\n",
    "            axs[1].imshow(sign_vis); axs[1].set_title(\"Delta sign (blue=neg, red=pos)\"); axs[1].axis('off')\n",
    "            plt.tight_layout()\n",
    "            if save_debug:\n",
    "                os.makedirs(\"val_debug\", exist_ok=True)\n",
    "                fig.savefig(f\"val_debug/delta_panel_epoch{epoch}.png\", dpi=200, bbox_inches='tight')\n",
    "            if show_debug: plt.show()\n",
    "            else:          plt.close(fig)\n",
    "\n",
    "    mean_l1m   = tot_l1m / max(1, n)\n",
    "    mean_ssimg = tot_ssim_global / max(1, n)\n",
    "    mean_ssimm = tot_ssim_mask / max(1, n)\n",
    "    mean_lpips = tot_lpips / max(1, n)\n",
    "\n",
    "    return mean_l1m, mean_ssimg, mean_ssimm, mean_lpips\n",
    "\n",
    "\n",
    "    \n",
    "def ssim_loss_on_bbox(pred, target, m_soft, *, use_ms=False, data_range=1.0, thresh=0.05, dilate_px=4):\n",
    "    B, C, H, W = pred.shape\n",
    "    losses = []\n",
    "    for b in range(B):\n",
    "        mb = m_soft[b, 0]\n",
    "        yy, xx = (mb > thresh).nonzero(as_tuple=True)\n",
    "        if yy.numel() == 0:\n",
    "            pb, tb = pred[b:b+1], target[b:b+1]\n",
    "        else:\n",
    "            y0, y1 = yy.min().item(), yy.max().item()+1\n",
    "            x0, x1 = xx.min().item(), xx.max().item()+1\n",
    "            if dilate_px > 0:\n",
    "                y0 = max(0, y0 - dilate_px); x0 = max(0, x0 - dilate_px)\n",
    "                y1 = min(H, y1 + dilate_px); x1 = min(W, x1 + dilate_px)\n",
    "            pb = pred[b:b+1, :, y0:y1, x0:x1]\n",
    "            tb = target[b:b+1, :, y0:y1, x0:x1]\n",
    "\n",
    "        # Clamp *only* for SSIM computation\n",
    "        pb = pb.clamp(0.0, 1.0)\n",
    "        tb = tb.clamp(0.0, 1.0)\n",
    "\n",
    "        ssim_fn = piq.ms_ssim if use_ms else piq.ssim\n",
    "        ssim_b = 1.0 - ssim_fn(pb, tb, data_range=1.0)\n",
    "        losses.append(ssim_b)\n",
    "    return torch.stack(losses).mean() if losses else torch.tensor(0.0, device=pred.device)\n",
    "\n",
    "    \n",
    "# ---------- training ----------\n",
    "def train_gan(\n",
    "    train_loader, val_loader, *,\n",
    "    epochs=20, out_dir=\"ckpts_gan\",\n",
    "    lr_G=2e-4, lr_D=1e-4,\n",
    "    adv_start_w=0.05,\n",
    "    lambda_l1=5.0, lambda_out_id=0.5,\n",
    "    use_perc=True, perceptual_fn=None,\n",
    "    use_ssim_loss=True,          # <--- enable differentiable SSIM loss\n",
    "    use_ms_ssim=False,           # <--- switch to MS-SSIM if you prefer\n",
    "    lambda_ssim=0.1,             # <--- weight for SSIM/MS-SSIM loss (start small: 0.05-0.2)\n",
    "    amp=True, device=\"cuda\",\n",
    "    resume_from=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Training with:\n",
    "      - L1(mask)\n",
    "      - identity outside mask\n",
    "      - GAN loss (hinge)\n",
    "      - optional perceptual loss\n",
    "      - optional (MS-)SSIM loss on nose ROI (bbox crop), differentiable via piq\n",
    "    Validation logs: L1(mask), SSIM(global), SSIM(mask)\n",
    "    Assumes images in [0,1]; if not, map before losses/metrics.\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # ----- build models -----\n",
    "    G = UNetNoseGenerator(in_ch=4, out_ch=3, base=64, depth=5, res_max=0.75).to(device)\n",
    "    D = PatchDiscriminator(in_ch=6, base=64).to(device)\n",
    "\n",
    "    # ----- opt -----\n",
    "    opt_G = torch.optim.AdamW(G.parameters(), lr=lr_G, betas=(0.5, 0.999))\n",
    "    opt_D = torch.optim.AdamW(D.parameters(), lr=lr_D, betas=(0.5, 0.999))\n",
    "\n",
    "    # ----- (optional) resume -----\n",
    "    start_ep = 1\n",
    "    best = float(\"inf\")\n",
    "    best_lpips = float(\"inf\")\n",
    "    best_path = None\n",
    "    last_path = None\n",
    "    if resume_from and os.path.isfile(resume_from):\n",
    "        ckpt = torch.load(resume_from, map_location=device)\n",
    "        G.load_state_dict(ckpt[\"G\"], strict=True)\n",
    "        D.load_state_dict(ckpt[\"D\"], strict=True)\n",
    "        if \"opt_G\" in ckpt: opt_G.load_state_dict(ckpt[\"opt_G\"])\n",
    "        if \"opt_D\" in ckpt: opt_D.load_state_dict(ckpt[\"opt_D\"])\n",
    "        start_ep = int(ckpt.get(\"epoch\", 0)) + 1\n",
    "        best = float(ckpt.get(\"val_l1m\", best))\n",
    "        print(f\"[resume] from {resume_from} → start_ep={start_ep}  best={best:.4f}\")\n",
    "    else:\n",
    "        if resume_from:\n",
    "            print(f\"[resume] file not found: {resume_from} (starting fresh)\")\n",
    "\n",
    "    # ----- schedulers -----\n",
    "    sch_G = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        opt_G, T_max=epochs, eta_min=lr_G*0.1, last_epoch=start_ep-2\n",
    "    )\n",
    "    sch_D = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        opt_D, T_max=epochs, eta_min=lr_D*0.1, last_epoch=start_ep-2\n",
    "    )\n",
    "\n",
    "    scaler_G = torch.amp.GradScaler('cuda', enabled=amp)\n",
    "    scaler_D = torch.amp.GradScaler('cuda', enabled=amp)\n",
    "\n",
    "    # ----- metrics (eval only) -----\n",
    "    ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "\n",
    "    best_path = None\n",
    "\n",
    "    for ep in range(start_ep, epochs+1):\n",
    "        G.train(); D.train()\n",
    "        pbar = tqdm(train_loader, desc=f\"train {ep}/{epochs}\")\n",
    "        adv_w = adv_start_w * min(1.0, ep/5.0)\n",
    "\n",
    "        for batch in pbar:\n",
    "            rgb    = batch[\"input\"].to(device)   # (B,3,H,W) in [0,1]\n",
    "            target = batch[\"target\"].to(device)  # (B,3,H,W)\n",
    "            mask   = batch[\"mask\"].to(device)    # (B,1,H,W)\n",
    "            inp    = torch.cat([rgb, mask], dim=1)\n",
    "\n",
    "            #hard    = dilate_mask_binary(mask, k=14, iters=2)\n",
    "            hard = mask\n",
    "            H, W = rgb.shape[-2:]\n",
    "            if hard.shape[-2:] != (H, W):\n",
    "                hard = F.interpolate(hard, size=(H, W), mode='nearest').clamp(0, 1)\n",
    "            #m_soft  = feather_mask(hard, k=5)\n",
    "            m_soft = hard\n",
    "            outside = 1.0 - m_soft\n",
    "\n",
    "            # ---- D step ----\n",
    "            opt_D.zero_grad(set_to_none=True)\n",
    "            with torch.amp.autocast('cuda', enabled=amp):\n",
    "                with torch.no_grad():\n",
    "                    fake = G(inp)\n",
    "                real_logits = D(torch.cat([rgb, target], dim=1))\n",
    "                fake_logits = D(torch.cat([rgb, fake],   dim=1))\n",
    "                # Hinge loss\n",
    "                d_loss = torch.relu(1. - real_logits).mean() + torch.relu(1. + fake_logits).mean()\n",
    "            scaler_D.scale(d_loss).backward()\n",
    "            scaler_D.step(opt_D)\n",
    "            scaler_D.update()\n",
    "\n",
    "            # ---- G step ----\n",
    "            opt_G.zero_grad(set_to_none=True)\n",
    "            with torch.amp.autocast('cuda', enabled=amp):\n",
    "                pred = G(inp)\n",
    "\n",
    "                # Reconstruction losses\n",
    "                l1_mask  = ((pred - target).abs() * m_soft).sum() / (m_soft.sum() + 1e-6)\n",
    "                l_out_id = ((pred - rgb).abs()    * outside).sum() / (outside.sum() + 1e-6)\n",
    "\n",
    "                # GAN loss\n",
    "                fake_logits_g = D(torch.cat([rgb, pred], dim=1))\n",
    "                adv_loss = -fake_logits_g.mean()\n",
    "\n",
    "                # Optional perceptual loss\n",
    "                perc_loss = perceptual_fn(pred, target, mask=m_soft) if (use_perc and perceptual_fn) else 0.0\n",
    "\n",
    "                # (MS-)SSIM loss on ROI (bbox around mask) using piq (differentiable)\n",
    "                if use_ssim_loss:\n",
    "                    # widen bbox a bit to include context\n",
    "                    ssim_loss = ssim_loss_on_bbox(pred, target, m_soft,use_ms=use_ms_ssim, data_range=1.0, thresh=0.05, dilate_px=4\n",
    "                                                )\n",
    "\n",
    "                    \n",
    "                else:\n",
    "                    ssim_loss = 0.0\n",
    "\n",
    "                g_loss = (lambda_l1 * l1_mask\n",
    "                          + lambda_out_id * l_out_id\n",
    "                          + adv_w * adv_loss\n",
    "                          + 4 * perc_loss\n",
    "                          + lambda_ssim * ssim_loss)\n",
    "            scaler_G.scale(g_loss).backward()\n",
    "            scaler_G.step(opt_G)\n",
    "            scaler_G.update()\n",
    "\n",
    "            # quick masked SSIM log (eval metric; no grads)\n",
    "            with torch.no_grad():\n",
    "                ssim_mask_step = ssim_metric(pred * m_soft, target * m_soft)\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"D\": f\"{d_loss.item():.3f}\",\n",
    "                \"G\": f\"{g_loss.item():.3f}\",\n",
    "                \"L1m\": f\"{l1_mask.item():.4f}\",\n",
    "                \"OID\": f\"{l_out_id.item():.4f}\",\n",
    "                \"ADV\": f\"{adv_loss.item():.4f}\",\n",
    "                \"Perc\": f\"{(perc_loss if isinstance(perc_loss, float) else perc_loss.item()):.4f}\",\n",
    "                \"SSIMm\": f\"{ssim_mask_step.item():.4f}\",\n",
    "                \"lrG\": f\"{sch_G.get_last_lr()[0]:.2e}\"\n",
    "            })\n",
    "\n",
    "        sch_G.step(); sch_D.step()\n",
    "\n",
    "        # ---- validation ----\n",
    "        val_l1m, val_ssim_global, val_ssim_mask, val_lpips = validate_epoch(\n",
    "    G, val_loader, device, ssim_metric,\n",
    "    epoch=ep, save_debug=True, show_debug=True,\n",
    "    perceptual_fn=perceptual_fn,   # <-- the SAME fn you use in training\n",
    ")\n",
    "\n",
    "        # ---- save ----\n",
    "        ckpt = {\n",
    "            \"epoch\": ep,\n",
    "            \"G\": G.state_dict(),\n",
    "            \"D\": D.state_dict(),\n",
    "            \"opt_G\": opt_G.state_dict(),\n",
    "            \"opt_D\": opt_D.state_dict(),\n",
    "            \"val_l1m\": val_l1m,\n",
    "            \"val_ssim_global\": val_ssim_global,\n",
    "            \"val_ssim_mask\": val_ssim_mask,\n",
    "            \"cfg\": {\n",
    "                \"lambda_l1\": lambda_l1,\n",
    "                \"lambda_out_id\": lambda_out_id,\n",
    "                \"adv_start_w\": adv_start_w,\n",
    "                \"use_ssim_loss\": use_ssim_loss,\n",
    "                \"use_ms_ssim\": use_ms_ssim,\n",
    "                \"lambda_ssim\": lambda_ssim\n",
    "            }\n",
    "        }\n",
    "        ckpt[\"cfg\"][\"perc_w\"] = perc_loss\n",
    "        \n",
    "        last_path = os.path.join(out_dir, \"last.pt\")\n",
    "        torch.save(ckpt, last_path)\n",
    "\n",
    "        saved_best = False\n",
    "        if val_l1m < best:  # keep L1(mask) as early-stopping criterion (or swap to -SSIM if desired)\n",
    "            best = val_l1m\n",
    "            best_path = os.path.join(out_dir, \"best_l1_mask.pt\")\n",
    "            torch.save(ckpt, best_path)\n",
    "            saved_best = True\n",
    "\n",
    "        print(f\"[epoch {ep}] L1(m)={val_l1m:.4f} | SSIM(g)={val_ssim_global:.4f} | \"\n",
    "      f\"SSIM(m)={val_ssim_mask:.4f} | LPIPS(m)={val_lpips:.4f} | \"\n",
    "      f\"best={best:.4f} | saved_best={saved_best}\")\n",
    "        if ep % 3 == 0:\n",
    "            save_vis_grid(\n",
    "    G, val_loader, device, out_dir, epoch=ep, n=6,\n",
    "    extra_names=[\n",
    "        \"WhatsApp Image 2025-07-12 at 1.31.43 AM (1)\",\n",
    "        \"WhatsApp Image 2025-07-12 at 1.36.06 AM (1)\",\n",
    "        \"WhatsApp Image 2025-07-12 at 1.36.12 AM\"\n",
    "    ],\n",
    "    max_extra=6\n",
    ")\n",
    "\n",
    "    return best_path or last_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[validate] counts: img=536 mask=513 target=536\n",
      "[validate] common triples: 513\n",
      "[validate] missing masks for 23 imgs, e.g. ['WhatsApp Image 2025-07-12 at 1.09.33 AM (1)', 'WhatsApp Image 2025-07-12 at 1.09.35 AM', 'WhatsApp Image 2025-07-12 at 1.30.13 AM (6)', 'WhatsApp Image 2025-07-12 at 1.33.47 AM (4)', 'WhatsApp Image 2025-07-12 at 1.38.10 AM']\n",
      "[validate] counts: img=67 mask=63 target=67\n",
      "[validate] common triples: 63\n",
      "[validate] missing masks for 4 imgs, e.g. ['WhatsApp Image 2025-07-12 at 5.46.30 PM', 'WhatsApp Image 2025-07-12 at 5.46.31 PM (2)', 'WhatsApp Image 2025-07-12 at 5.53.17 PM (1)', 'WhatsApp Image 2025-07-12 at 6.20.03 PM']\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Use the keep-size dataset (returns RGB (3ch) + mask (1ch) separately)\n",
    "train_ds = NoseFolderDataset(\n",
    "    \"data_splits/train/input\",\n",
    "    \"data_splits/train/mask_input_new\",\n",
    "    \"data_splits/train/target\"\n",
    ")\n",
    "val_ds = NoseFolderDataset(\n",
    "    \"data_splits/val/input\",\n",
    "    \"data_splits/val/mask_input_new\",\n",
    "    \"data_splits/val/target\"\n",
    ")\n",
    "\n",
    "MULT = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=2, shuffle=True, num_workers=0, pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "    collate_fn=lambda b: collate_keep_aspect(b, multiple=MULT)   # no token cap needed\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=2, shuffle=False, num_workers=0, pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "    collate_fn=lambda b: collate_keep_aspect(b, multiple=MULT)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lpips import LPIPS\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def make_perceptual_fn(lpips_model, masked=True, bbox=True, bbox_dilate=4, min_side=64):\n",
    "    \"\"\"\n",
    "    Returns: perceptual_fn(pred, target, mask) -> scalar tensor with grad w.r.t. pred\n",
    "    - masked: neutralize outside nose with SOFT mask (better boundary behavior)\n",
    "    - bbox: crop to per-image tight ROI of (mask>0.05), dilated by bbox_dilate\n",
    "    - min_side: upsize tiny ROIs to stabilize LPIPS\n",
    "    \"\"\"\n",
    "    lpips_model.eval()\n",
    "    for p in lpips_model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    lpips_device = next(lpips_model.parameters()).device\n",
    "\n",
    "    def _to_m1p1(x):  # [0,1] -> [-1,1]\n",
    "        return x.mul(2.0).sub(1.0)\n",
    "\n",
    "    def _bbox_from_mask(mb, thresh=0.05):\n",
    "        # mb: [H,W] soft mask\n",
    "        yy, xx = (mb > thresh).nonzero(as_tuple=True)\n",
    "        if yy.numel() == 0:\n",
    "            return None\n",
    "        y0, y1 = yy.min().item(), yy.max().item() + 1\n",
    "        x0, x1 = xx.min().item(), xx.max().item() + 1\n",
    "        return y0, y1, x0, x1\n",
    "\n",
    "    def _pad_bbox(y0, y1, x0, x1, H, W, pad):\n",
    "        if pad <= 0: return y0, y1, x0, x1\n",
    "        return (max(0, y0 - pad), min(H, y1 + pad),\n",
    "                max(0, x0 - pad), min(W, x1 + pad))\n",
    "\n",
    "    def _maybe_resize(t):\n",
    "        # t: [1,3,h,w]; upsize tiny crops for LPIPS stability\n",
    "        _, _, h, w = t.shape\n",
    "        s = min(h, w)\n",
    "        if s >= min_side:\n",
    "            return t\n",
    "        scale = float(min_side) / max(1, s)\n",
    "        nh = max(min_side, int(round(h * scale)))\n",
    "        nw = max(min_side, int(round(w * scale)))\n",
    "        return F.interpolate(t, size=(nh, nw), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    def perceptual_fn(pred, target, mask=None):\n",
    "        # ensure device/dtype\n",
    "        pred   = pred.to(lpips_device).float()\n",
    "        target = target.to(lpips_device).float()\n",
    "        if mask is None:\n",
    "            mask = torch.ones(pred.size(0), 1, pred.size(2), pred.size(3),\n",
    "                              device=pred.device, dtype=pred.dtype)\n",
    "        else:\n",
    "            mask = mask.to(lpips_device).float()\n",
    "\n",
    "        # neutralize outside with SOFT mask\n",
    "        if masked:\n",
    "            pred_eff   = pred * mask + target * (1.0 - mask)\n",
    "            target_eff = target\n",
    "        else:\n",
    "            pred_eff, target_eff = pred, target\n",
    "\n",
    "        B, _, H, W = pred.shape\n",
    "        vals = []\n",
    "\n",
    "        for b in range(B):\n",
    "            pb = pred_eff[b:b+1]\n",
    "            tb = target_eff[b:b+1]\n",
    "\n",
    "            if bbox:\n",
    "                mb = mask[b, 0]  # [H,W]\n",
    "                bb = _bbox_from_mask(mb)\n",
    "                if bb is not None:\n",
    "                    y0, y1, x0, x1 = _pad_bbox(*bb, H, W, bbox_dilate)\n",
    "                    pb = pb[:, :, y0:y1, x0:x1]\n",
    "                    tb = tb[:, :, y0:y1, x0:x1]\n",
    "                # else: empty mask -> fall back to full image\n",
    "\n",
    "            # stabilize LPIPS on tiny crops; AMP-safe to keep FP32\n",
    "            pb = _maybe_resize(pb).float()\n",
    "            tb = _maybe_resize(tb).float()\n",
    "\n",
    "            d = lpips_model(_to_m1p1(pb), _to_m1p1(tb))  # [1] or [1,1,1,1]\n",
    "            vals.append(d.view(-1))  # shape [1]\n",
    "\n",
    "        return torch.stack(vals).mean() if vals else torch.zeros((), device=pred.device)\n",
    "\n",
    "    return perceptual_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: /Users/nikethdamodaran/Documents/Rhinoplasty_Post/.venv/lib/python3.11/site-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    }
   ],
   "source": [
    "lpips_model = LPIPS(net=\"vgg\").to(device)\n",
    "perceptual_fn = make_perceptual_fn(lpips_model, masked=True, bbox=True, bbox_dilate=4, min_side=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[resume] from ckpts_Unet_PatchGan_Res_SSIMv1_test_no_clamp_with_new_mask/last.pt → start_ep=81  best=0.2421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train 81/90:   0%|          | 0/257 [00:00<?, ?it/s]/Users/nikethdamodaran/Documents/Rhinoplasty_Post/.venv/lib/python3.11/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "train 81/90:   0%|          | 0/257 [00:44<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 17.88 GiB, other allocations: 13.58 MiB, max allowed: 18.13 GiB). Tried to allocate 475.00 MiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m      5\u001b[39m     resume_ckpt = \u001b[33m\"\u001b[39m\u001b[33mckpts_Unet_PatchGan_Res_SSIMv1_test_no_clamp_with_new_mask/last.pt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     best_path = \u001b[43mtrain_gan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m90\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                              \u001b[49m\u001b[38;5;66;43;03m# how many epochs you want\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mckpts_Unet_PatchGan_Res_SSIMv1_test_no_clamp_with_new_mask\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# where to save checkpoints\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamp\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume_from\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_ckpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# pass checkpoint path or None\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlambda_l1\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m9.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlambda_out_id\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_perc\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mperceptual_fn\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mperceptual_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m# set True if you pass a perceptual_fn\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_ssim_loss\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m                     \u001b[49m\u001b[38;5;66;43;03m# enable SSIM/MS-SSIM loss\u001b[39;49;00m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_ms_ssim\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m                      \u001b[49m\u001b[38;5;66;43;03m# True if you want MS-SSIM instead\u001b[39;49;00m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlambda_ssim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m                         \u001b[49m\u001b[38;5;66;43;03m# weight for SSIM loss\u001b[39;49;00m\n\u001b[32m     22\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest checkpoint saved at:\u001b[39m\u001b[33m\"\u001b[39m, best_path)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 438\u001b[39m, in \u001b[36mtrain_gan\u001b[39m\u001b[34m(train_loader, val_loader, epochs, out_dir, lr_G, lr_D, adv_start_w, lambda_l1, lambda_out_id, use_perc, perceptual_fn, use_ssim_loss, use_ms_ssim, lambda_ssim, amp, device, resume_from)\u001b[39m\n\u001b[32m    436\u001b[39m opt_G.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    437\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.amp.autocast(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m, enabled=amp):\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m     pred = \u001b[43mG\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    440\u001b[39m     \u001b[38;5;66;03m# Reconstruction losses\u001b[39;00m\n\u001b[32m    441\u001b[39m     l1_mask  = ((pred - target).abs() * m_soft).sum() / (m_soft.sum() + \u001b[32m1e-6\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Rhinoplasty_Post/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Rhinoplasty_Post/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 129\u001b[39m, in \u001b[36mUNetNoseGenerator.forward\u001b[39m\u001b[34m(self, inp, return_full)\u001b[39m\n\u001b[32m    127\u001b[39m u3 = \u001b[38;5;28mself\u001b[39m.up3(u2, x3)\n\u001b[32m    128\u001b[39m u4 = \u001b[38;5;28mself\u001b[39m.up4(u3, x2)\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m u5 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mup5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m raw   = \u001b[38;5;28mself\u001b[39m.outc(u5)               \u001b[38;5;66;03m# [B,3,H,W]\u001b[39;00m\n\u001b[32m    133\u001b[39m delta = raw\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Rhinoplasty_Post/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Rhinoplasty_Post/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mUp.forward\u001b[39m\u001b[34m(self, x, skip)\u001b[39m\n\u001b[32m     52\u001b[39m     x = F.pad(x, (\u001b[32m0\u001b[39m, diffX, \u001b[32m0\u001b[39m, diffY))\n\u001b[32m     53\u001b[39m x = torch.cat([skip, x], dim=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Rhinoplasty_Post/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Rhinoplasty_Post/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mConvBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Rhinoplasty_Post/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Rhinoplasty_Post/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Rhinoplasty_Post/.venv/lib/python3.11/site-packages/torch/nn/modules/container.py:244\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Rhinoplasty_Post/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Rhinoplasty_Post/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Rhinoplasty_Post/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:434\u001b[39m, in \u001b[36mSiLU.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Rhinoplasty_Post/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2374\u001b[39m, in \u001b[36msilu\u001b[39m\u001b[34m(input, inplace)\u001b[39m\n\u001b[32m   2372\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(silu, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, inplace=inplace)\n\u001b[32m   2373\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m-> \u001b[39m\u001b[32m2374\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43msilu_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2375\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch._C._nn.silu(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: MPS backend out of memory (MPS allocated: 17.88 GiB, other allocations: 13.58 MiB, max allowed: 18.13 GiB). Tried to allocate 475.00 MiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    \n",
    "    resume_ckpt = \"ckpts_Unet_PatchGan_Res_SSIMv1_test_no_clamp_with_new_mask/last.pt\"\n",
    "\n",
    "    best_path = train_gan(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=90,                              # how many epochs you want\n",
    "    out_dir=\"ckpts_Unet_PatchGan_Res_SSIMv1_test_no_clamp_with_new_mask\",    # where to save checkpoints\n",
    "    amp=True,\n",
    "    device=device,\n",
    "    resume_from=resume_ckpt,                # pass checkpoint path or None\n",
    "    lambda_l1=9.5,\n",
    "    lambda_out_id=0.5,\n",
    "    use_perc=True,\n",
    "    perceptual_fn = perceptual_fn,# set True if you pass a perceptual_fn\n",
    "    use_ssim_loss=True,                     # enable SSIM/MS-SSIM loss\n",
    "    use_ms_ssim=False,                      # True if you want MS-SSIM instead\n",
    "    lambda_ssim=0.1                         # weight for SSIM loss\n",
    ")\n",
    "\n",
    "print(\"Best checkpoint saved at:\", best_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded generator from ckpts_Unet_PatchGan_Res_SSIMv1_test_no_clamp_with_new_mask/best_l1_mask.pt\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "\n",
    "# path to your trained checkpoint\n",
    "ckpt_path = \"ckpts_Unet_PatchGan_Res_SSIMv1_test_no_clamp_with_new_mask/best_l1_mask.pt\" \n",
    "\n",
    "# rebuild generator architecture  (same as training)\n",
    "G = UNetNoseGenerator(in_ch=4, out_ch=3, base=64, depth=5, res_max=0.75).to(device)\n",
    "\n",
    "# load weights\n",
    "ckpt = torch.load(ckpt_path, map_location=device)\n",
    "G.load_state_dict(ckpt[\"G\"])\n",
    "G.eval()\n",
    "\n",
    "print(\"Loaded generator from\", ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /Users/nikethdamodaran/Documents/Rhinoplasty_Post/.venv/lib/python3.11/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'count_pairs': 65,\n",
       " 'avg_SSIM': 0.8065791120895973,\n",
       " 'avg_PSNR': 22.71080200488751,\n",
       " 'avg_LPIPS': 0.18107184710410926,\n",
       " 'unmatched_pred_files': 0}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from metrics_eval import evaluate_folders\n",
    "summary_model_output, df = evaluate_folders(\"/Users/nikethdamodaran/Documents/Rhinoplasty_Post/data_splits/test/outputs/outputs\", \"/Users/nikethdamodaran/Documents/Rhinoplasty_Post/data_splits/test/target\", save_csv=\"out/metrics.csv\")\n",
    "summary_model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikethdamodaran/Documents/Rhinoplasty_Post/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/nikethdamodaran/Documents/Rhinoplasty_Post/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /Users/nikethdamodaran/Documents/Rhinoplasty_Post/.venv/lib/python3.11/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'count_pairs': 65,\n",
       " 'avg_SSIM': 0.8062570113402147,\n",
       " 'avg_PSNR': 22.687408740703876,\n",
       " 'avg_LPIPS': 0.18137581359881622,\n",
       " 'unmatched_pred_files': 0}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from metrics_eval import evaluate_folders\n",
    "summary_postprocessing, df = evaluate_folders(\"/Users/nikethdamodaran/Documents/Rhinoplasty_Post/data_splits/test/Final_post_processing 2\", \"/Users/nikethdamodaran/Documents/Rhinoplasty_Post/data_splits/test/target\", save_csv=\"out/metrics.csv\")\n",
    "summary_postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikethdamodaran/Documents/Rhinoplasty_Post/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/nikethdamodaran/Documents/Rhinoplasty_Post/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /Users/nikethdamodaran/Documents/Rhinoplasty_Post/.venv/lib/python3.11/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'count_pairs': 63,\n",
       " 'avg_SSIM': 0.8021409894738879,\n",
       " 'avg_PSNR': 22.566000075567338,\n",
       " 'avg_LPIPS': 0.19168458926299262,\n",
       " 'unmatched_pred_files': 0}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from metrics_eval import evaluate_folders\n",
    "summary_model_output_val, df = evaluate_folders(\"/Users/nikethdamodaran/Documents/Rhinoplasty_Post/data_splits/val/outputs/outputs\", \"/Users/nikethdamodaran/Documents/Rhinoplasty_Post/data_splits/val/target\", save_csv=\"out/metrics.csv\")\n",
    "summary_model_output_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from: /Users/nikethdamodaran/Documents/Rhinoplasty_Post/metrics_eval.py\n",
      "True\n",
      "['evaluate_folders', 'evaluate_folders_masked']\n"
     ]
    }
   ],
   "source": [
    "import sys, importlib, os\n",
    "\n",
    "# Make sure the folder that *contains* metrics_eval.py is on sys.path\n",
    "sys.path.append(\"/Users/nikethdamodaran/Documents/Rhinoplasty_Post\")\n",
    "\n",
    "import metrics_eval\n",
    "print(\"Loaded from:\", metrics_eval.__file__)\n",
    "\n",
    "# Reload to pick up edits if you had imported it earlier\n",
    "importlib.reload(metrics_eval)\n",
    "\n",
    "# Sanity check: does the function exist?\n",
    "print(hasattr(metrics_eval, \"evaluate_folders_masked\"))\n",
    "print([n for n in dir(metrics_eval) if \"evaluate_folders\" in n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikethdamodaran/Documents/Rhinoplasty_Post/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/nikethdamodaran/Documents/Rhinoplasty_Post/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /Users/nikethdamodaran/Documents/Rhinoplasty_Post/.venv/lib/python3.11/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'count_masked_pairs': 65,\n",
       " 'avg_SSIM_mask': 0.7516825139522553,\n",
       " 'avg_PSNR_mask': 18.382675904494064,\n",
       " 'avg_LPIPS_mask': 0.2503171723622542,\n",
       " 'skipped_no_target_or_pred_match': 0,\n",
       " 'skipped_no_mask': 0,\n",
       " 'skipped_too_small_or_empty_mask': 0}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from metrics_eval import evaluate_folders_masked\n",
    "summary_model_output, df = evaluate_folders_masked(\"/Users/nikethdamodaran/Documents/Rhinoplasty_Post/data_splits/test/outputs/outputs\", \"/Users/nikethdamodaran/Documents/Rhinoplasty_Post/data_splits/test/target\", \"/Users/nikethdamodaran/Documents/Rhinoplasty_Post/data_splits/test/mask_input_new\",save_csv=\"out/metrics.csv\")\n",
    "summary_model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikethdamodaran/Documents/Rhinoplasty_Post/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/nikethdamodaran/Documents/Rhinoplasty_Post/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /Users/nikethdamodaran/Documents/Rhinoplasty_Post/.venv/lib/python3.11/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'count_masked_pairs': 65,\n",
       " 'avg_SSIM_mask': 0.7446762772706839,\n",
       " 'avg_PSNR_mask': 18.603207969665526,\n",
       " 'avg_LPIPS_mask': 0.23911003665282177,\n",
       " 'skipped_no_target_or_pred_match': 0,\n",
       " 'skipped_no_mask': 0,\n",
       " 'skipped_too_small_or_empty_mask': 0}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from metrics_eval import evaluate_folders_masked\n",
    "summary_postprocessing, df = evaluate_folders_masked(\"/Users/nikethdamodaran/Documents/Rhinoplasty_Post/data_splits/test/Final_post_processing\", \"/Users/nikethdamodaran/Documents/Rhinoplasty_Post/data_splits/test/target\", \"/Users/nikethdamodaran/Documents/Rhinoplasty_Post/data_splits/test/mask_input_new\",save_csv=\"out/metrics.csv\")\n",
    "summary_postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /Users/nikethdamodaran/Documents/Rhinoplasty_Post/.venv/lib/python3.11/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'count_pairs': 65,\n",
       " 'avg_SSIM': 0.8062570113402147,\n",
       " 'avg_PSNR': 22.687408740703876,\n",
       " 'avg_LPIPS': 0.18137581359881622,\n",
       " 'unmatched_pred_files': 0}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from metrics_eval import evaluate_folders\n",
    "\n",
    "summary_postprocessing, df = evaluate_folders(\"/Users/nikethdamodaran/Documents/Rhinoplasty_Post/data_splits/test/Final_post_processing 2\", \"/Users/nikethdamodaran/Documents/Rhinoplasty_Post/data_splits/test/target\", save_csv=\"out/metrics.csv\")\n",
    "summary_postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def _dilate_mask(mask_t: torch.Tensor, dilate_px: int) -> torch.Tensor:\n",
    "    if dilate_px <= 0:\n",
    "        return mask_t\n",
    "    k = dilate_px * 2 + 1\n",
    "    return F.max_pool2d(mask_t, kernel_size=k, stride=1, padding=dilate_px)\n",
    "\n",
    "@torch.no_grad()\n",
    "def _erode_mask(mask_t: torch.Tensor, erode_px: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Binary/soft erosion via max-pool trick on the inverted mask.\n",
    "    Keeps shape; works for [B,1,H,W] masks in [0,1].\n",
    "    \"\"\"\n",
    "    if erode_px <= 0:\n",
    "        return mask_t\n",
    "    k = erode_px * 2 + 1\n",
    "    inv = 1.0 - mask_t\n",
    "    inv = F.max_pool2d(inv, kernel_size=k, stride=1, padding=erode_px)\n",
    "    return (1.0 - inv).clamp(0, 1)\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def _gauss1d(sigma: float, device=None, dtype=None):\n",
    "    k = max(3, int(math.ceil(sigma * 6)))\n",
    "    if k % 2 == 0:\n",
    "        k += 1\n",
    "    x = torch.arange(k, device=device, dtype=dtype) - (k // 2)\n",
    "    g = torch.exp(-(x**2) / (2 * sigma * sigma))\n",
    "    g = g / g.sum()\n",
    "    return g  # [k], 1D vector\n",
    "\n",
    "@torch.no_grad()\n",
    "def _feather_mask(mask_t: torch.Tensor, feather_px: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Gaussian feather with safe padding for small H/W.\n",
    "    mask_t: [B,1,H,W] in [0,1]\n",
    "    \"\"\"\n",
    "    if feather_px <= 0:\n",
    "        return mask_t\n",
    "\n",
    "    B, C, H, W = mask_t.shape\n",
    "    device, dtype = mask_t.device, mask_t.dtype\n",
    "\n",
    "    # sigma ~ radius/2\n",
    "    sigma = max(0.5, feather_px / 2.0)\n",
    "    g = _gauss1d(sigma, device=device, dtype=dtype)   # [k]\n",
    "    kH = g.numel()\n",
    "    kW = kH\n",
    "\n",
    "    # Build separable kernels with CORRECT conv2d weight shapes:\n",
    "    # conv2d expects [out_c, in_c, kH, kW]\n",
    "    k_v = g.view(1, 1, kH, 1)   # vertical: [1,1,k,1]\n",
    "    k_h = g.view(1, 1, 1, kW)   # horizontal: [1,1,1,k]\n",
    "\n",
    "    # Safe pads (reflect needs pad <= dim-1)\n",
    "    pad_y = min(kH // 2, max(H - 1, 0))\n",
    "    pad_x = min(kW // 2, max(W - 1, 0))\n",
    "    pad_mode = \"reflect\" if (pad_y > 0 and pad_x > 0) else \"replicate\"\n",
    "\n",
    "    # Ensure contiguous (helps on MPS)\n",
    "    m = mask_t.contiguous()\n",
    "\n",
    "    # Vertical blur\n",
    "    m = F.pad(m, (0, 0, pad_y, pad_y), mode=pad_mode)\n",
    "    m = F.conv2d(m, k_v, bias=None, stride=1, padding=0, groups=1)\n",
    "\n",
    "    # Horizontal blur\n",
    "    m = F.pad(m, (pad_x, pad_x, 0, 0), mode=pad_mode)\n",
    "    m = F.conv2d(m, k_h, bias=None, stride=1, padding=0, groups=1)\n",
    "\n",
    "    return m.clamp(0, 1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def _lock_core(mask_orig: torch.Tensor, mask_soft: torch.Tensor, lock_core_px: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Restore the inner (eroded) core of the original mask to full opacity after feathering.\n",
    "    lock_core_px ≈ how many pixels of solid center to keep.\n",
    "    \"\"\"\n",
    "    if lock_core_px <= 0:\n",
    "        return mask_soft\n",
    "    core = _erode_mask(mask_orig, lock_core_px).clamp(0, 1)\n",
    "    # Ensure core stays fully opaque:\n",
    "    return torch.maximum(mask_soft, core)\n",
    "\n",
    "# ---------------- integrate into your function ----------------\n",
    "@torch.no_grad()\n",
    "def infer_single_with_metrics(\n",
    "    G,\n",
    "    img_path: str,\n",
    "    mask_path: str,\n",
    "    target_path: str = None,\n",
    "    out_path: str = \"output.png\",\n",
    "    dilate_px: int = 0,\n",
    "    feather_px: int = 6,        # NEW\n",
    "    lock_core_px: int = 4,      # NEW\n",
    "    clamp_output: bool = True,\n",
    "    device: torch.device = None,\n",
    "):\n",
    "    if device is None:\n",
    "        device = next(G.parameters()).device\n",
    "\n",
    "    # ---- Load RGB ----\n",
    "    rgb = Image.open(img_path).convert(\"RGB\")\n",
    "    rgb_t = T.ToTensor()(rgb).unsqueeze(0).to(device)  # [1,3,H,W]\n",
    "\n",
    "    # ---- Load mask ----\n",
    "    mask = Image.open(mask_path).convert(\"L\")\n",
    "    mask_t = T.ToTensor()(mask).unsqueeze(0).to(device)\n",
    "    if mask_t.shape[-2:] != rgb_t.shape[-2:]:\n",
    "        mask_t = F.interpolate(mask_t, size=rgb_t.shape[-2:], mode=\"nearest\")\n",
    "\n",
    "    # ---- Forward ----\n",
    "    inp = torch.cat([rgb_t, mask_t], dim=1)\n",
    "    try:\n",
    "        out, full_rgb, pred_mask = G(inp, return_full=True)\n",
    "    except TypeError:\n",
    "        full_rgb = G(inp)\n",
    "        pred_mask = mask_t\n",
    "\n",
    "    # ---- Mask refine: dilate -> feather -> lock core ----\n",
    "    m = pred_mask.clamp(0, 1)\n",
    "    m = _dilate_mask(m, dilate_px).clamp(0, 1)\n",
    "    m = _feather_mask(m, feather_px).clamp(0, 1)\n",
    "    m = _lock_core(pred_mask, m, lock_core_px).clamp(0, 1)\n",
    "\n",
    "    # ---- Blend with soft mask ----\n",
    "    blended = rgb_t + m * (full_rgb - rgb_t)\n",
    "    if clamp_output:\n",
    "        blended = blended.clamp(0, 1)\n",
    "\n",
    "    # ---- Save + (optional metrics) ----\n",
    "    os.makedirs(os.path.dirname(out_path) or \".\", exist_ok=True)\n",
    "    save_image(blended, out_path)\n",
    "    print(f\"[OK] Saved blended output → {out_path}\")\n",
    "\n",
    "    metrics = {}\n",
    "    if target_path and os.path.exists(target_path):\n",
    "        target = Image.open(target_path).convert(\"RGB\")\n",
    "        tgt_t = T.ToTensor()(target).unsqueeze(0).to(device)\n",
    "        if tgt_t.shape[-2:] != blended.shape[-2:]:\n",
    "            tgt_t = F.interpolate(tgt_t, size=blended.shape[-2:], mode=\"bilinear\")\n",
    "        ssim_global = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "        ssim_masked = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "        lpips_fn = LPIPS(net=\"vgg\").to(device)\n",
    "        ssim_g = ssim_global(blended, tgt_t).item()\n",
    "        blended_m = blended * mask_t\n",
    "        tgt_m = tgt_t * mask_t\n",
    "        ssim_m = ssim_masked(blended_m, tgt_m).item()\n",
    "        lp = lpips_fn(blended, tgt_t).mean().item()\n",
    "        metrics = {\"SSIM_global\": ssim_g, \"SSIM_mask\": ssim_m, \"LPIPS\": lp}\n",
    "        print(\"\\n==== Metrics ====\")\n",
    "        print(f\"Global SSIM : {ssim_g:.4f}\")\n",
    "        print(f\"Mask   SSIM : {ssim_m:.4f}\")\n",
    "        print(f\"LPIPS Score: {lp:.4f}\")\n",
    "        print(\"=================\\n\")\n",
    "    else:\n",
    "        print(\"[INFO] Target not provided — metrics skipped.\")\n",
    "\n",
    "    return blended, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved blended output → test_blended_clamped_perc_train_feather.png\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: /Users/nikethdamodaran/Documents/Rhinoplasty_Post/.venv/lib/python3.11/site-packages/lpips/weights/v0.1/vgg.pth\n",
      "\n",
      "==== Metrics ====\n",
      "Global SSIM : 0.8149\n",
      "Mask   SSIM : 0.9918\n",
      "LPIPS Score: 0.2311\n",
      "=================\n",
      "\n",
      "{'SSIM_global': 0.8148698806762695, 'SSIM_mask': 0.9918382167816162, 'LPIPS': 0.2311442792415619}\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "\n",
    "# paths\n",
    "img_path=\"data_splits/train/input/WhatsApp Image 2025-07-12 at 12.39.53 AM.jpeg\"\n",
    "mask_path=\"data_splits/train/mask_input_new/WhatsApp Image 2025-07-12 at 12.39.53 AM.png\"\n",
    "target_path=\"data_splits/train/target/WhatsApp Image 2025-07-12 at 12.39.53 AM.jpeg\"\n",
    "out_path=\"test_blended_clamped_perc_train_feather.png\"\n",
    "\n",
    "# run inference\n",
    "blended, metrics = infer_single_with_metrics(\n",
    "    G,                     # your trained generator\n",
    "    img_path=img_path,\n",
    "    mask_path=mask_path,\n",
    "    target_path=target_path,  # optional\n",
    "    out_path=out_path,\n",
    "    dilate_px =1,           # expand mask a little (optional)\n",
    "    feather_px=10,          # soften edges (main control)\n",
    "    lock_core_px=4,        # keep center fully opaque\n",
    "    clamp_output=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# show metrics if target was provided\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved cleaned image to clean_black_bg.png\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def make_background_black(img_path, out_path=\"output_black_bg.png\", dark_thresh=60):\n",
    "    \"\"\"\n",
    "    Make background purely black while keeping the subject intact.\n",
    "\n",
    "    Args:\n",
    "        img_path (str): Path to input image.\n",
    "        out_path (str): Path to save output image.\n",
    "        dark_thresh (int): Pixel intensity threshold to treat as background (default=60).\n",
    "    \"\"\"\n",
    "    # Read image\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Threshold to detect dark background\n",
    "    mask = gray < dark_thresh\n",
    "\n",
    "    # Optional smoothing (to avoid sharp edges near hair)\n",
    "    mask = cv2.GaussianBlur(mask.astype(np.uint8) * 255, (9, 9), 0)\n",
    "    mask = mask > 127\n",
    "\n",
    "    # Create a black background\n",
    "    black_bg = np.zeros_like(img)\n",
    "\n",
    "    # Blend the subject onto black background\n",
    "    result = np.where(mask[..., None], black_bg, img)\n",
    "\n",
    "    # Convert back to BGR for saving\n",
    "    result_bgr = cv2.cvtColor(result, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(out_path, result_bgr)\n",
    "\n",
    "    print(f\"[OK] Saved cleaned image to {out_path}\")\n",
    "\n",
    "# Example usage:\n",
    "make_background_black(\"data_splits/train/input/WhatsApp Image 2025-07-12 at 12.39.53 AM.jpeg\", \"clean_black_bg.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import save_image\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def _bbox_from_mask_bin(mask_np):\n",
    "    ys, xs = np.where(mask_np > 0.5)\n",
    "    if len(xs) == 0: return None\n",
    "    return int(xs.min()), int(ys.min()), int(xs.max()), int(ys.max())\n",
    "\n",
    "def _area_ratio(mask_np):\n",
    "    H, W = mask_np.shape\n",
    "    return float((mask_np > 0.5).sum()) / float(H*W + 1e-6)\n",
    "\n",
    "def _feather_mask(mask_np, feather_px=10):\n",
    "    import cv2\n",
    "    if feather_px <= 0: \n",
    "        return mask_np.astype(np.float32)\n",
    "    m = mask_np.astype(np.float32)\n",
    "    k = 2*feather_px + 1\n",
    "    m = cv2.GaussianBlur(m, (k,k), 0)\n",
    "    return np.clip(m, 0, 1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def _dilate_mask(mask_t: torch.Tensor, dilate_px: int) -> torch.Tensor:\n",
    "    if dilate_px <= 0:\n",
    "        return mask_t\n",
    "    k = dilate_px * 2 + 1\n",
    "    return F.max_pool2d(mask_t, kernel_size=k, stride=1, padding=dilate_px)\n",
    "\n",
    "def _pad_to_multiple(t: torch.Tensor, multiple: int):\n",
    "    \"\"\"Pad (B,C,H,W) to nearest >= multiple on H and W. Returns padded, (pad_l,pad_r,pad_t,pad_b).\"\"\"\n",
    "    B,C,H,W = t.shape\n",
    "    Ht = (H + multiple - 1) // multiple * multiple\n",
    "    Wt = (W + multiple - 1) // multiple * multiple\n",
    "    pad_b = Ht - H\n",
    "    pad_r = Wt - W\n",
    "    padded = F.pad(t, (0, pad_r, 0, pad_b))  # left=0,right=pad_r, top=0,bottom=pad_b\n",
    "    return padded, (0, pad_r, 0, pad_b)\n",
    "\n",
    "def _unpad(t: torch.Tensor, pads):\n",
    "    _, pad_r, _, pad_b = pads\n",
    "    if pad_r == 0 and pad_b == 0:\n",
    "        return t\n",
    "    return t[..., :t.shape[-2]-pad_b, :t.shape[-1]-pad_r]\n",
    "\n",
    "# ---------- main ----------\n",
    "@torch.no_grad()\n",
    "def infer_single_scale_norm_keep_aspect(\n",
    "    G,\n",
    "    img_path: str,\n",
    "    mask_path: str,\n",
    "    target_path: str = None,     # optional for metrics\n",
    "    out_path: str = \"output.png\",\n",
    "    r_w: float = 0.218,          # canonical fractions from your training set\n",
    "    r_h: float = 0.142,\n",
    "    margin_frac: float = 0.12,\n",
    "    big_threshold: float = 0.11, # router on mask area\n",
    "    force_normalize: bool = True,\n",
    "    multiple: int = 32,          # pad stride like your DataLoader\n",
    "    feather_px: int = 10,\n",
    "    dilate_px: int = 0,\n",
    "    clamp_output: bool = True,\n",
    "    device: torch.device = None,\n",
    "    metrics=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Keeps aspect ratio. Chooses a crop so nose ~ (r_w, r_h) of crop, pads crop to `multiple`,\n",
    "    runs G on [rgb,mask], unpads, blends back with feathering. Falls back to full-frame if not 'big'.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = next(G.parameters()).device\n",
    "\n",
    "    # --- load image and mask (keep full-res) ---\n",
    "    rgb = Image.open(img_path).convert(\"RGB\")\n",
    "    W_full, H_full = rgb.size\n",
    "    rgb_t = T.ToTensor()(rgb).unsqueeze(0).to(device)  # [1,3,H,W]\n",
    "\n",
    "    mask = Image.open(mask_path).convert(\"L\")\n",
    "    mask_t = T.ToTensor()(mask).unsqueeze(0)           # CPU [1,1,?,?]\n",
    "    if mask_t.shape[-2:] != rgb_t.shape[-2:]:\n",
    "        mask_t = F.interpolate(mask_t, size=rgb_t.shape[-2:], mode=\"nearest\")\n",
    "    mask_np = (mask_t.squeeze(0).squeeze(0).numpy() > 0.5).astype(np.uint8)\n",
    "\n",
    "    # --- router ---\n",
    "    ratio = _area_ratio(mask_np)\n",
    "    do_norm = force_normalize or (ratio > big_threshold)\n",
    "\n",
    "    if do_norm:\n",
    "        # 1) nose bbox\n",
    "        nb = _bbox_from_mask_bin(mask_np)\n",
    "        if nb is None:\n",
    "            do_norm = False\n",
    "        else:\n",
    "            x1n, y1n, x2n, y2n = nb\n",
    "            nose_w = x2n - x1n + 1\n",
    "            nose_h = y2n - y1n + 1\n",
    "\n",
    "            # 2) choose crop so nose fraction ≈ (r_w, r_h)\n",
    "            crop_w = int(np.ceil(nose_w / max(1e-6, r_w)))\n",
    "            crop_h = int(np.ceil(nose_h / max(1e-6, r_h)))\n",
    "            crop_w = int(crop_w * (1 + 2*margin_frac))\n",
    "            crop_h = int(crop_h * (1 + 2*margin_frac))\n",
    "\n",
    "            cx = (x1n + x2n) // 2\n",
    "            cy = (y1n + y2n) // 2\n",
    "            x1 = max(0, cx - crop_w // 2)\n",
    "            y1 = max(0, cy - crop_h // 2)\n",
    "            x2 = min(W_full-1, x1 + crop_w - 1)\n",
    "            y2 = min(H_full-1, y1 + crop_h - 1)\n",
    "\n",
    "            # 3) slice crop (no resize), then pad to stride\n",
    "            rgb_crop = rgb_t[:, :, y1:y2+1, x1:x2+1]          # [1,3,hc,wc]\n",
    "            mask_crop = mask_t[:, :, y1:y2+1, x1:x2+1].to(device)  # [1,1,hc,wc]\n",
    "            rgb_pad, pads = _pad_to_multiple(rgb_crop, multiple)\n",
    "            mask_pad, _    = _pad_to_multiple(mask_crop, multiple)\n",
    "\n",
    "            # 4) forward G on padded crop\n",
    "            inp = torch.cat([rgb_pad, mask_pad], dim=1)       # [1,4,Ht,Wt]\n",
    "            try:\n",
    "                _, full_rgb_pad, pred_mask_pad = G(inp, return_full=True)\n",
    "            except TypeError:\n",
    "                full_rgb_pad = G(inp)\n",
    "                pred_mask_pad = mask_pad\n",
    "\n",
    "            # 5) unpad, dilate+blend IN CROP SPACE\n",
    "            full_rgb_u = _unpad(full_rgb_pad, pads)\n",
    "            pred_mask_u = _unpad(pred_mask_pad, pads)\n",
    "            mask_d = _dilate_mask(pred_mask_u, dilate_px).clamp(0,1)\n",
    "            blended_crop = rgb_crop + mask_d * (full_rgb_u - rgb_crop)\n",
    "            if clamp_output:\n",
    "                blended_crop = blended_crop.clamp(0,1)\n",
    "\n",
    "            # 6) paste back with feather on original frame\n",
    "            paste_np = blended_crop.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "            mask_core = (mask_crop.squeeze(0).squeeze(0).cpu().numpy() > 0.5).astype(np.uint8)\n",
    "            m_soft = _feather_mask(mask_core, feather_px=feather_px)[..., None]\n",
    "\n",
    "            base_np = rgb_t.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "            roi = base_np[y1:y2+1, x1:x2+1]\n",
    "            base_np[y1:y2+1, x1:x2+1] = m_soft * paste_np + (1.0 - m_soft) * roi\n",
    "            blended_full = torch.from_numpy(base_np).permute(2,0,1).unsqueeze(0).to(device)\n",
    "\n",
    "            os.makedirs(os.path.dirname(out_path) or \".\", exist_ok=True)\n",
    "            save_image(blended_full, out_path)\n",
    "            print(f\"[OK] Saved (normalized keep-aspect) → {out_path}\")\n",
    "\n",
    "            out_metrics = {}\n",
    "            if metrics and target_path and os.path.exists(target_path):\n",
    "                from torchmetrics.image.ssim import StructuralSimilarityIndexMeasure\n",
    "                from lpips import LPIPS\n",
    "                target = T.ToTensor()(Image.open(target_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "                if target.shape[-2:] != blended_full.shape[-2:]:\n",
    "                    target = F.interpolate(target, size=blended_full.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "                ssim_g = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)(blended_full, target).item()\n",
    "                # build full-frame mask aligned to paste region for masked SSIM\n",
    "                mask_full = torch.zeros_like(mask_t).to(device)\n",
    "                mask_full[:,:,y1:y2+1, x1:x2+1] = mask_crop\n",
    "                ssim_m = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)(blended_full*mask_full, target*mask_full).item()\n",
    "                lp = LPIPS(net=\"vgg\").to(device)(blended_full, target).mean().item()\n",
    "                out_metrics = {\"SSIM_global\": ssim_g, \"SSIM_mask\": ssim_m, \"LPIPS\": lp}\n",
    "                print(out_metrics)\n",
    "\n",
    "            return blended_full, out_metrics\n",
    "\n",
    "    # ---- fallback: run your original full-frame path with padding only ----\n",
    "    # (useful when mask empty or not big)\n",
    "    rgb_pad, pads = _pad_to_multiple(rgb_t, multiple)\n",
    "    mask_pad, _   = _pad_to_multiple(mask_t.to(device), multiple)\n",
    "    inp = torch.cat([rgb_pad, mask_pad], dim=1)\n",
    "    try:\n",
    "        _, full_rgb_pad, pred_mask_pad = G(inp, return_full=True)\n",
    "    except TypeError:\n",
    "        full_rgb_pad = G(inp)\n",
    "        pred_mask_pad = mask_pad\n",
    "\n",
    "    full_rgb = _unpad(full_rgb_pad, pads)\n",
    "    pred_mask = _unpad(pred_mask_pad, pads)\n",
    "    mask_d = _dilate_mask(pred_mask, dilate_px).clamp(0,1)\n",
    "    blended = rgb_t + mask_d * (full_rgb - rgb_t)\n",
    "    if clamp_output:\n",
    "        blended = blended.clamp(0,1)\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_path) or \".\", exist_ok=True)\n",
    "    save_image(blended, out_path)\n",
    "    print(f\"[OK] Saved (direct keep-aspect) → {out_path}\")\n",
    "\n",
    "    return blended, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved (normalized keep-aspect) → test_norm_keep_aspect.png\n"
     ]
    }
   ],
   "source": [
    "blended, _ = infer_single_scale_norm_keep_aspect(\n",
    "    G,\n",
    "    img_path=\"data_splits/train/input/WhatsApp Image 2025-07-12 at 1.38.18 AM.jpeg\",\n",
    "    mask_path=\"data_splits/train/mask_input_new/WhatsApp Image 2025-07-12 at 1.38.18 AM.png\",\n",
    "    target_path=\"data_splits/train/target/WhatsApp Image 2025-07-12 at 1.38.18 AM.jpeg\",\n",
    "    out_path=\"test_norm_keep_aspect.png\",\n",
    "    r_w=0.218, r_h=0.142,\n",
    "    force_normalize=True,   # <- always normalize\n",
    "    multiple=32, feather_px=10, dilate_px=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved (normalized keep-aspect) → test_norm_keep_aspect.png\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikethdamodaran/Documents/Rhinoplasty_Post/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/nikethdamodaran/Documents/Rhinoplasty_Post/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /Users/nikethdamodaran/Documents/Rhinoplasty_Post/.venv/lib/python3.11/site-packages/lpips/weights/v0.1/vgg.pth\n",
      "{'SSIM_global': 0.7669007182121277, 'SSIM_mask': 0.9888899326324463, 'LPIPS': 0.26196587085723877}\n"
     ]
    }
   ],
   "source": [
    "_ = infer_single_scale_norm_keep_aspect(\n",
    "    G,\n",
    "    img_path=\"data_splits/train/input/WhatsApp Image 2025-07-12 at 1.38.18 AM.jpeg\",\n",
    "    mask_path=\"data_splits/train/mask_input_new/WhatsApp Image 2025-07-12 at 1.38.18 AM.png\",\n",
    "    target_path=\"data_splits/train/target/WhatsApp Image 2025-07-12 at 1.38.18 AM.jpeg\",\n",
    "    out_path=\"test_norm_keep_aspect.png\",\n",
    "    r_w=0.218, r_h=0.142,\n",
    "    force_normalize=True,     # try True to validate; then set False and use big_threshold\n",
    "    multiple=32,              # same stride as your collate\n",
    "    feather_px=10,\n",
    "    dilate_px=0,\n",
    "    metrics=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[validate] counts: img=68 mask=65 target=68\n",
      "[validate] common triples: 65\n",
      "[validate] missing masks for 3 imgs, e.g. ['WhatsApp Image 2025-07-12 at 1.36.12 AM (2)', 'WhatsApp Image 2025-07-12 at 5.38.14 PM (1)', 'WhatsApp Image 2025-07-12 at 5.53.22 PM (1)']\n"
     ]
    }
   ],
   "source": [
    "test_ds = NoseFolderDataset(\n",
    "    \"/workspace/data_splits/test/input\",\n",
    "    \"/workspace/data_splits/test/mask_input_new\",\n",
    "    \"/workspace/data_splits/test/target\"\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds, batch_size=2, shuffle=False, num_workers=2, pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "    collate_fn=lambda b: collate_keep_aspect(b, multiple=32)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: /usr/local/lib/python3.12/dist-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[infer]: 100%|██████████| 257/257 [06:37<00:00,  1.55s/it, SSIMg=0.831, SSIMm=0.995, LPIPS=0.179]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Test Metrics ====\n",
      "Global SSIM : 0.8361\n",
      "Mask   SSIM : 0.9939\n",
      "LPIPS Score: 0.1734\n",
      "======================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "G.eval().to(device)\n",
    "\n",
    "metrics = infer_with_loader(\n",
    "    G,\n",
    "    train_loader,\n",
    "    out_dir=\"/workspace/train_outputs/outputs\",\n",
    "    grid_dir=\"/workspace/train_outputs/grids\",\n",
    "    device=device,\n",
    "    dilate_px=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchmetrics.image.ssim import StructuralSimilarityIndexMeasure\n",
    "from lpips import LPIPS\n",
    "from tqdm import tqdm\n",
    "\n",
    "@torch.no_grad()\n",
    "def _dilate_mask(mask_t: torch.Tensor, dilate_px: int) -> torch.Tensor:\n",
    "    \"\"\"Dilate [B,1,H,W] mask by `dilate_px` pixels using max-pooling.\"\"\"\n",
    "    if dilate_px <= 0:\n",
    "        return mask_t\n",
    "    k = dilate_px * 2 + 1\n",
    "    return F.max_pool2d(mask_t, kernel_size=k, stride=1, padding=dilate_px)\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer_with_loader(\n",
    "    G,\n",
    "    test_loader,\n",
    "    out_dir=\"test_outputs\",\n",
    "    grid_dir=\"test_outputs/grids\",\n",
    "    device=None,\n",
    "    dilate_px: int = 0,       # same as infer_single()\n",
    "    clamp_output: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Inference with mask dilation & re-blending:\n",
    "        blended = rgb + mask_dilated * (full_rgb - rgb)\n",
    "    Saves predictions, triptych grids, and computes SSIM/LPIPS metrics.\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    os.makedirs(grid_dir, exist_ok=True)\n",
    "\n",
    "    if device is None:\n",
    "        device = next(G.parameters()).device\n",
    "\n",
    "    # ---- Metrics ----\n",
    "    ssim_global = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "    ssim_masked = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "    lpips_fn = LPIPS(net=\"vgg\").to(device)\n",
    "\n",
    "    ssim_g_list, ssim_m_list, lpips_list = [], [], []\n",
    "\n",
    "    pbar = tqdm(test_loader, desc=\"[infer]\")\n",
    "\n",
    "    for batch in pbar:\n",
    "        rgb   = batch[\"input\"].to(device)   # [B,3,H,W]\n",
    "        tgt   = batch[\"target\"].to(device)  # [B,3,H,W]\n",
    "        mask  = batch[\"mask\"].to(device)    # [B,1,H,W]\n",
    "        fnames = batch.get(\"fname\", [f\"img_{i}.png\" for i in range(rgb.size(0))])\n",
    "\n",
    "        # ---- Forward pass ----\n",
    "        inp = torch.cat([rgb, mask], dim=1)\n",
    "        try:\n",
    "            out, full_rgb, pred_mask = G(inp, return_full=True)\n",
    "        except TypeError:\n",
    "            full_rgb = G(inp)\n",
    "            pred_mask = mask\n",
    "\n",
    "        # ---- Mask dilation & blending ----\n",
    "        mask_d = pred_mask\n",
    "        blended = rgb + mask_d * (full_rgb - rgb)\n",
    "        if clamp_output:\n",
    "            blended = blended.clamp(0, 1)\n",
    "\n",
    "        # ---- Per-sample metrics, saving, and grids ----\n",
    "        for b, fname in enumerate(fnames):\n",
    "            stem = os.path.splitext(fname)[0]\n",
    "            save_path = os.path.join(out_dir, f\"{stem}.png\")\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            save_image(blended[b], save_path)\n",
    "\n",
    "            # --- Metrics ---\n",
    "            ssim_g = ssim_global(blended[b:b+1], tgt[b:b+1]).item()\n",
    "\n",
    "            blended_m = blended[b:b+1] * mask[b:b+1]\n",
    "            tgt_m = tgt[b:b+1] * mask[b:b+1]\n",
    "            ssim_m = ssim_masked(blended_m, tgt_m).item()\n",
    "\n",
    "            lp = lpips_fn(blended[b:b+1], tgt[b:b+1]).mean().item()\n",
    "\n",
    "            ssim_g_list.append(ssim_g)\n",
    "            ssim_m_list.append(ssim_m)\n",
    "            lpips_list.append(lp)\n",
    "\n",
    "            # --- Grid: (input | blended | target) ---\n",
    "            grid = make_grid(\n",
    "                torch.cat([rgb[b:b+1], blended[b:b+1], tgt[b:b+1]], dim=0),\n",
    "                nrow=3, padding=4, normalize=True, value_range=(0, 1)\n",
    "            )\n",
    "            grid_path = os.path.join(grid_dir, f\"{stem}_grid.jpg\")\n",
    "            to_pil_image(grid).save(grid_path, quality=95, subsampling=0)\n",
    "\n",
    "        # ---- Live metrics summary ----\n",
    "        pbar.set_postfix({\n",
    "            \"SSIMg\": f\"{np.mean(ssim_g_list[-5:]):.3f}\",\n",
    "            \"SSIMm\": f\"{np.mean(ssim_m_list[-5:]):.3f}\",\n",
    "            \"LPIPS\": f\"{np.mean(lpips_list[-5:]):.3f}\"\n",
    "        })\n",
    "\n",
    "    # ---- Summary ----\n",
    "    ssim_g_mean = float(np.mean(ssim_g_list)) if ssim_g_list else 0.0\n",
    "    ssim_m_mean = float(np.mean(ssim_m_list)) if ssim_m_list else 0.0\n",
    "    lpips_mean  = float(np.mean(lpips_list))  if lpips_list  else 0.0\n",
    "\n",
    "    print(\"\\n==== Test Metrics ====\")\n",
    "    print(f\"Global SSIM : {ssim_g_mean:.4f}\")\n",
    "    print(f\"Mask   SSIM : {ssim_m_mean:.4f}\")\n",
    "    print(f\"LPIPS Score: {lpips_mean:.4f}\")\n",
    "    print(\"======================\\n\")\n",
    "\n",
    "    return {\n",
    "        \"ssim_global\": ssim_g_list,\n",
    "        \"ssim_mask\": ssim_m_list,\n",
    "        \"lpips\": lpips_list,\n",
    "        \"mean\": {\n",
    "            \"ssim_global\": ssim_g_mean,\n",
    "            \"ssim_mask\": ssim_m_mean,\n",
    "            \"lpips\": lpips_mean,\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, cv2, os, numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer_lowfreq_darkening(\n",
    "    model,\n",
    "    input_path,\n",
    "    mask_path,\n",
    "    out_path,\n",
    "    device=None,\n",
    "    inpaint_radius=3,\n",
    "    sigma_px=7,\n",
    "    dark_gain=1.0,\n",
    "    feather_px=4,\n",
    "):\n",
    "    \"\"\"\n",
    "    Inference with:\n",
    "      - Base cleaning (inpaint old pixels inside mask)\n",
    "      - Hard replacement with model output\n",
    "      - Low-frequency luminance darkening (preserve shape)\n",
    "      - Small feather for seamless transition\n",
    "\n",
    "    Args:\n",
    "        model         : trained generator\n",
    "        input_path    : original image path\n",
    "        mask_path     : binary mask (white = region to replace)\n",
    "        out_path      : where to save final image (e.g. '/tmp/out.png')\n",
    "        device        : torch device\n",
    "        inpaint_radius: OpenCV inpainting radius\n",
    "        sigma_px      : Gaussian blur radius for low-freq delta\n",
    "        dark_gain     : scale factor for darkening strength\n",
    "        feather_px    : boundary feather width\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # --- Load image & mask ---\n",
    "    img_bgr = cv2.imread(input_path, cv2.IMREAD_COLOR)\n",
    "    if img_bgr is None:\n",
    "        raise FileNotFoundError(f\"Cannot read: {input_path}\")\n",
    "    mask_gray = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if mask_gray is None:\n",
    "        raise FileNotFoundError(f\"Cannot read: {mask_path}\")\n",
    "\n",
    "    mask_bin = (mask_gray > 127).astype(np.uint8)\n",
    "\n",
    "    rgb = to_tensor(cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)).unsqueeze(0).to(device)\n",
    "    mask = torch.from_numpy(mask_bin).float().unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "    # --- Forward ---\n",
    "    inp = torch.cat([rgb, mask], dim=1)\n",
    "    try:\n",
    "        _, full_rgb, _ = model(inp, return_full=True)\n",
    "    except TypeError:\n",
    "        full_rgb = model(inp)\n",
    "\n",
    "    mask_hard = (mask > 0.5).float()\n",
    "\n",
    "    # --- Clean base (remove old pixels inside mask) ---\n",
    "    img_u8 = (rgb[0].permute(1,2,0).cpu().numpy()*255).astype(np.uint8)\n",
    "    m_u8   = (mask_hard[0,0].cpu().numpy()*255).astype(np.uint8)\n",
    "    base_clean_u8 = cv2.inpaint(img_u8, m_u8, inpaint_radius, cv2.INPAINT_TELEA)\n",
    "    base_clean = torch.from_numpy(base_clean_u8.astype(np.float32)/255.).permute(2,0,1).unsqueeze(0).to(device)\n",
    "\n",
    "    # --- Hard replace (no old pixels) ---\n",
    "    hard = base_clean * (1 - mask_hard) + full_rgb * mask_hard\n",
    "\n",
    "    # --- Delta & low-frequency luminance shaping ---\n",
    "    delta = full_rgb - rgb\n",
    "    toY = lambda x: (0.299*x[:,0]+0.587*x[:,1]+0.114*x[:,2]).unsqueeze(1)\n",
    "    dY = toY(delta)\n",
    "\n",
    "    dY_np = dY[0,0].cpu().numpy().astype(np.float32)\n",
    "    k = max(3, 2*(sigma_px//2)+1)\n",
    "    dY_low = cv2.GaussianBlur(dY_np, (k,k), sigmaX=sigma_px)\n",
    "    dY_low_t = torch.from_numpy(dY_low).float().unsqueeze(0).unsqueeze(0).to(device) * dark_gain\n",
    "\n",
    "    # RGB→YUV utilities\n",
    "    def rgb_to_yuv(x):\n",
    "        r,g,b = x[:,0:1], x[:,1:2], x[:,2:3]\n",
    "        Y = 0.299*r + 0.587*g + 0.114*b\n",
    "        U = -0.14713*r - 0.28886*g + 0.436*b\n",
    "        V = 0.615*r - 0.51499*g - 0.10001*b\n",
    "        return torch.cat([Y,U,V], dim=1)\n",
    "    def yuv_to_rgb(yuv):\n",
    "        Y,U,V = yuv[:,0:1], yuv[:,1:2], yuv[:,2:3]\n",
    "        r = Y + 1.13983*V\n",
    "        g = Y - 0.39465*U - 0.58060*V\n",
    "        b = Y + 2.03211*U\n",
    "        return torch.cat([r,g,b], dim=1)\n",
    "\n",
    "    hard_yuv = rgb_to_yuv(hard)\n",
    "    hard_yuv[:,0:1] = hard_yuv[:,0:1] * (1 - mask_hard) + (hard_yuv[:,0:1] + dY_low_t) * mask_hard\n",
    "    shaped = yuv_to_rgb(hard_yuv).clamp(0,1)\n",
    "\n",
    "    # --- Feather boundary ---\n",
    "    if feather_px > 0:\n",
    "        kf = feather_px*2 + 1\n",
    "        mask_soft = F.avg_pool2d(mask_hard, kf, stride=1, padding=feather_px).clamp(0,1)\n",
    "        shaped = base_clean * (1 - mask_soft) + shaped * mask_soft\n",
    "\n",
    "    # --- Save ---\n",
    "    out_img = (shaped[0].permute(1,2,0).cpu().numpy()*255).astype(np.uint8)\n",
    "    out_bgr = cv2.cvtColor(out_img, cv2.COLOR_RGB2BGR)\n",
    "    os.makedirs(os.path.dirname(out_path) or \".\", exist_ok=True)\n",
    "    cv2.imwrite(out_path, out_bgr)\n",
    "    print(f\"[OK] Saved low-freq darkened output → {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved low-freq darkened output → /Users/nikethdamodaran/Documents/Rhinoplasty_Post/test_outputs_/test_clean_feather.png\n"
     ]
    }
   ],
   "source": [
    "infer_lowfreq_darkening(\n",
    "    model=G,\n",
    "    input_path=\"/Users/nikethdamodaran/Documents/Rhinoplasty_Post/data_splits/train/input/WhatsApp Image 2025-07-12 at 1.06.34 AM.jpeg\",\n",
    "    mask_path=\"/Users/nikethdamodaran/Documents/Rhinoplasty_Post/data_splits/train/mask_input_new/WhatsApp Image 2025-07-12 at 1.06.34 AM.png\",\n",
    "    out_path=\"/Users/nikethdamodaran/Documents/Rhinoplasty_Post/test_outputs_/test_clean_feather.png\",\n",
    "    device=device,\n",
    "    sigma_px=7,\n",
    "    dark_gain=1.0,\n",
    "    feather_px=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved cleaned + feathered output → /Users/nikethdamodaran/Documents/Rhinoplasty_Post/test_outputs/test_clean_feather.png\n"
     ]
    }
   ],
   "source": [
    "infer_clean_feather(\n",
    "    model=G,\n",
    "    input_path=\"/Users/nikethdamodaran/Documents/Rhinoplasty_Post/data_splits/train/input/WhatsApp Image 2025-07-12 at 1.06.34 AM.jpeg\",\n",
    "    mask_path=\"/Users/nikethdamodaran/Documents/Rhinoplasty_Post/data_splits/train/mask_input_new/WhatsApp Image 2025-07-12 at 1.06.34 AM.png\",\n",
    "    out_path=\"/Users/nikethdamodaran/Documents/Rhinoplasty_Post/test_outputs/test_clean_feather.png\",\n",
    "    device=device,\n",
    "    inpaint_radius=7,\n",
    "    feather_px=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to Export a Complete Data set with Predictions , Input , Masks, Target for Train, Val and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path as osp\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import utils as vutils\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------- utility: ensure dirs ----------\n",
    "def _ensure_dirs(root):\n",
    "    for sub in [\"pred\", \"mask\", \"target\", \"input\", \"grids\"]:\n",
    "        os.makedirs(osp.join(root, sub), exist_ok=True)\n",
    "\n",
    "# ---------- utility: save single image tensor ----------\n",
    "def _save_tensor_img(t, path):\n",
    "    \"\"\"\n",
    "    t: [C,H,W] in [0,1]; supports 1ch or 3ch\n",
    "    \"\"\"\n",
    "    vutils.save_image(t.clamp(0,1), path)\n",
    "\n",
    "def _mask_to_rgb(mask_1ch):\n",
    "    \"\"\"[1,H,W] -> [3,H,W] red overlay-friendly\"\"\"\n",
    "    if mask_1ch.size(0) == 1:\n",
    "        return mask_1ch.repeat(3,1,1)\n",
    "    return mask_1ch\n",
    "\n",
    "# ---------- main export for one split ----------\n",
    "@torch.no_grad()\n",
    "def export_split_for_refiner(G, loader, out_dir_split, *, device=\"cuda\", ext=\".png\"):\n",
    "    \"\"\"\n",
    "    Writes per-sample files:\n",
    "      out_dir_split/pred/<basename>.png\n",
    "      out_dir_split/mask/<basename>.png\n",
    "      out_dir_split/target/<basename>.png\n",
    "      out_dir_split/input/<basename>.png\n",
    "    And per-batch grids:\n",
    "      out_dir_split/grids/grid_XXXX.png\n",
    "\n",
    "    Uses 'orig_hw' to crop away padding if present.\n",
    "    \"\"\"\n",
    "    G.eval()\n",
    "    _ensure_dirs(out_dir_split)\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"[export] → {Path(out_dir_split).name}\")\n",
    "    for bi, batch in enumerate(pbar):\n",
    "        rgb    = batch[\"input\"].to(device)      # [B,3,Ht,Wt] in [0,1]\n",
    "        target = batch.get(\"target\")\n",
    "        target = target.to(device) if (target is not None) else None\n",
    "        mask   = batch[\"mask\"].to(device)       # [B,1,Ht,Wt]\n",
    "        orig_hw = batch.get(\"orig_hw\", None)    # [B,2] (H, W)\n",
    "        files = batch.get(\"files\", None)        # list of tuples (input_file, mask_file, target_file)\n",
    "\n",
    "        # build basenames\n",
    "        if files is not None:\n",
    "            basenames = [Path(f[0]).stem if f and f[0] else f\"sample_{bi:04d}_{i:02d}\"\n",
    "                         for i, f in enumerate(files)]\n",
    "        elif \"fname\" in batch:\n",
    "            basenames = [Path(n).stem for n in batch[\"fname\"]]\n",
    "        elif \"input_file\" in batch:\n",
    "            basenames = [Path(p).stem for p in batch[\"input_file\"]]\n",
    "        else:\n",
    "            B = rgb.size(0)\n",
    "            basenames = [f\"sample_{bi:04d}_{i:02d}\" for i in range(B)]\n",
    "\n",
    "        # forward\n",
    "        inp = torch.cat([rgb, mask], dim=1)\n",
    "        pred = G(inp)  # assume model outputs [0,1] or close; clamp when saving\n",
    "\n",
    "        # unpad/crop back to original size per sample if orig_hw is provided\n",
    "        B = pred.size(0)\n",
    "        tiles_for_grid = []\n",
    "        for i in range(B):\n",
    "            base = basenames[i]\n",
    "\n",
    "            # slice to per-sample\n",
    "            pred_i = pred[i].detach()\n",
    "            mask_i = mask[i].detach()\n",
    "            rgb_i  = rgb[i].detach()\n",
    "            tgt_i  = target[i].detach() if target is not None else None\n",
    "\n",
    "            if orig_hw is not None:\n",
    "                H, W = int(orig_hw[i,0].item()), int(orig_hw[i,1].item())\n",
    "                pred_i = pred_i[:, :H, :W]\n",
    "                mask_i = mask_i[:, :H, :W]\n",
    "                rgb_i  = rgb_i[:,  :H, :W]\n",
    "                if tgt_i is not None:\n",
    "                    tgt_i = tgt_i[:, :H, :W]\n",
    "\n",
    "            # save per-sample\n",
    "            _save_tensor_img(pred_i, osp.join(out_dir_split, \"pred\",   base + ext))\n",
    "            _save_tensor_img(mask_i, osp.join(out_dir_split, \"mask\",   base + ext))\n",
    "            _save_tensor_img(rgb_i,  osp.join(out_dir_split, \"input\",  base + ext))\n",
    "            if tgt_i is not None:\n",
    "                _save_tensor_img(tgt_i, osp.join(out_dir_split, \"target\", base + ext))\n",
    "\n",
    "            # collect tiles for grid: Input | Pred | Target? | Mask\n",
    "            tiles_for_grid.append(rgb_i.clamp(0,1))\n",
    "            tiles_for_grid.append(pred_i.clamp(0,1))\n",
    "            if tgt_i is not None:\n",
    "                tiles_for_grid.append(tgt_i.clamp(0,1))\n",
    "            # mask as 3ch for consistent grid tile width\n",
    "            tiles_for_grid.append(_mask_to_rgb(mask_i.clamp(0,1)))\n",
    "\n",
    "        # make one grid per batch\n",
    "        try:\n",
    "            cols = 4 if target is not None else 3  # Input/Pred/Target/Mask or Input/Pred/Mask\n",
    "            grid = make_grid(tiles_for_grid, nrow=cols, padding=2)\n",
    "            grid_path = osp.join(out_dir_split, \"grids\", f\"grid_{bi:04d}.png\")\n",
    "            vutils.save_image(grid, grid_path)\n",
    "        except Exception as e:\n",
    "            # Fallback: save first few samples separately\n",
    "            for i in range(min(B, 4)):\n",
    "                base = basenames[i]\n",
    "                vutils.save_image(pred[i].clamp(0,1), osp.join(out_dir_split, \"grids\", f\"{base}_pred.png\"))\n",
    "\n",
    "    print(f\"✓ Exported split to: {out_dir_split}\")\n",
    "\n",
    "# ---------- convenience wrapper for all splits ----------\n",
    "@torch.no_grad()\n",
    "def export_refiner_dataset(G, loaders_dict, out_root, *, device=\"cuda\", ext=\".png\"):\n",
    "    \"\"\"\n",
    "    loaders_dict: {\"train\": train_loader, \"val\": val_loader, \"test\": test_loader}\n",
    "    \"\"\"\n",
    "    for split, loader in loaders_dict.items():\n",
    "        out_dir_split = osp.join(out_root, split)\n",
    "        export_split_for_refiner(G, loader, out_dir_split, device=device, ext=ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Loaded generator from ckpts_Unet_PatchGan_Res_SSIMv1_test_no_clamp/best_l1_mask.pt\n",
      "[validate] counts: img=536 mask=513 target=536\n",
      "[validate] common triples: 513\n",
      "[validate] missing masks for 23 imgs, e.g. ['WhatsApp Image 2025-07-12 at 1.09.33 AM (1)', 'WhatsApp Image 2025-07-12 at 1.09.35 AM', 'WhatsApp Image 2025-07-12 at 1.30.13 AM (6)', 'WhatsApp Image 2025-07-12 at 1.33.47 AM (4)', 'WhatsApp Image 2025-07-12 at 1.38.10 AM']\n",
      "[validate] counts: img=67 mask=63 target=67\n",
      "[validate] common triples: 63\n",
      "[validate] missing masks for 4 imgs, e.g. ['WhatsApp Image 2025-07-12 at 5.46.30 PM', 'WhatsApp Image 2025-07-12 at 5.46.31 PM (2)', 'WhatsApp Image 2025-07-12 at 5.53.17 PM (1)', 'WhatsApp Image 2025-07-12 at 6.20.03 PM']\n",
      "[validate] counts: img=68 mask=65 target=68\n",
      "[validate] common triples: 65\n",
      "[validate] missing masks for 3 imgs, e.g. ['WhatsApp Image 2025-07-12 at 1.36.12 AM (2)', 'WhatsApp Image 2025-07-12 at 5.38.14 PM (1)', 'WhatsApp Image 2025-07-12 at 5.53.22 PM (1)']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[export] → train: 100%|██████████| 257/257 [11:48<00:00,  2.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Exported split to: refiner_dataset_v2/train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[export] → val: 100%|██████████| 32/32 [01:28<00:00,  2.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Exported split to: refiner_dataset_v2/val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[export] → test: 100%|██████████| 33/33 [01:29<00:00,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Exported split to: refiner_dataset_v2/test\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    import torch\n",
    "    from torch.utils.data import DataLoader\n",
    "    import multiprocessing as mp\n",
    "\n",
    "    # ---- device selection (cuda -> mps -> cpu) ----\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    # (Optional) small speed bump\n",
    "    torch.backends.cudnn.benchmark = (device == \"cuda\")\n",
    "\n",
    "    # ---- paths ----\n",
    "    TRAIN_IMG = \"/workspace/data_splits/train/input\"\n",
    "    TRAIN_MSK = \"/workspace/data_splits/train/mask_input\"\n",
    "    TRAIN_TGT = \"/workspace/data_splits/train/target\"\n",
    "\n",
    "    VAL_IMG   = \"/workspace/data_splits/val/input\"\n",
    "    VAL_MSK   = \"/workspace/data_splits/val/mask_input\"\n",
    "    VAL_TGT   = \"/workspace/data_splits/val/target\"\n",
    "\n",
    "    TEST_IMG  = \"/workspace/data_splits/test/input\"\n",
    "    TEST_MSK  = \"/workspace/data_splits/test/mask_input\"\n",
    "    TEST_TGT  = \"/workspace/data_splits/test/target\"\n",
    "\n",
    "    # ---- rebuild + load generator ----\n",
    "    ckpt_path = \"ckpts_Unet_PatchGan_Res_SSIMv1_test_no_clamp/best_l1_mask.pt\"\n",
    "    G = UNetNoseGenerator(in_ch=4, out_ch=3, base=64, depth=5, res_max=0.75).to(device).eval()\n",
    "\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    G.load_state_dict(ckpt[\"G\"], strict=True)\n",
    "    print(\"Loaded generator from\", ckpt_path)\n",
    "\n",
    "    # ---- datasets + loaders ----\n",
    "    train_ds = NoseFolderDataset(TRAIN_IMG, TRAIN_MSK, TRAIN_TGT)\n",
    "    val_ds   = NoseFolderDataset(VAL_IMG,   VAL_MSK,   VAL_TGT)\n",
    "    test_ds  = NoseFolderDataset(TEST_IMG,  TEST_MSK,  TEST_TGT)\n",
    "\n",
    "    bs = 2\n",
    "    num_cpu = max(2, mp.cpu_count() // 2)\n",
    "    workers = min(8, num_cpu)\n",
    "    pin_mem = (device == \"cuda\")\n",
    "\n",
    "    collate = lambda b: collate_keep_aspect(b, multiple=32)  # matches UNet stride\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=bs, shuffle=False,\n",
    "                              num_workers=workers, pin_memory=pin_mem,\n",
    "                              persistent_workers=(workers > 0), collate_fn=collate)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=bs, shuffle=False,\n",
    "                              num_workers=workers, pin_memory=pin_mem,\n",
    "                              persistent_workers=(workers > 0), collate_fn=collate)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=bs, shuffle=False,\n",
    "                              num_workers=workers, pin_memory=pin_mem,\n",
    "                              persistent_workers=(workers > 0), collate_fn=collate)\n",
    "\n",
    "    # ---- export all splits ----\n",
    "    out_root = \"refiner_dataset_v2\"\n",
    "    export_refiner_dataset(\n",
    "        G,\n",
    "        {\"train\": train_loader, \"val\": val_loader, \"test\": test_loader},\n",
    "        out_root,\n",
    "        device=device,\n",
    "        ext=\".png\"\n",
    "    )\n",
    "\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8073816,
     "sourceId": 12803539,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
